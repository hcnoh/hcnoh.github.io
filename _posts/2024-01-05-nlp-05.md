---
layout: post
use_math: true
title: "[자연어 처리] 5. GPT"
date: 2024-01-05 00:00:00
tagline: "자연어 처리에서 생성형 거대 언어 모델의 대표격인 GPT에 대한 소개"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-05-nlp-05
---

이번 포스트에서는 최근의 생성형 AI의 트렌드를 이끌고 있는 생성형 거대 언어 모델의 대표 주자인 `GPT`에 대해서 다룬다. GPT는 현재 세계에서 가장 인기가 많고 가장 많이 활용되는 생성형 모델이다. 이러한 GPT의 인기를 이끄는 것은 역시 사람 수준의 언어 이해력 및 구사력일 것이다. 이번 포스트를 통해서 GPT는 어떤 구조를 가진 모델이고 어떤 방식으로 훈련이 되었는지, 그리고 어떻게 활용될 수 있을지를 다뤄보고자 한다.

## 목차
- [GPT 모델 구조](#gpt-모델-구조)
- [GPT와 BERT의 차이](#gpt와-bert의-차이)
- [GPT 프레임워크](#gpt-프레임워크)
    - [비지도 사전 훈련](#비지도-사전-훈련)
    - [지도 파인 튜닝](#지도-파인-튜닝)
- [GPT의 발전 과정](#gpt의-발전-과정)
    - [GPT-1](#gpt-1)
    - [GPT-2](#gpt-2)
    - [GPT-3](#gpt-3)
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## GPT 모델 구조
GPT는 기본적으로 사람의 언어 구사를 흉내내도록 학습된 딥러닝 모델이라고 볼 수 있다. 이를 위하여 GPT는 짧은 입력 텍스트를 받아서 더욱 많은 양의 텍스트를 생성하도록 구성되어 있다. 기본적으로 GPT는 `Autoregressive Model` 모델이다. 즉, 자기 자신이 생성한 텍스트를 다시 자신의 입력으로 받아 이를 바탕으로 다시 출력 텍스트를 생성하는 자기회귀적인 특성을 지닌다.

아래는 GPT의 정의를 정리한 것이다:

**Definition)**
- GPT는 거대한 양의 텍스트 데이터를 통해서 사전 훈련(Pre-trained)된 언어 모델이며 다양한 분야의 언어 태스크를 수행할 수 있다.
- GPT는 딥러닝을 통해 주어진 텍스트 기반의 입력에 대해서 사람과 유사한 텍스트를 생성할 수 있는 언어 모델이다.
- GPT는 OpenAI에서 지능형 시스템을 위해 개발한 언어 모델이며 ChatGPT 프로젝트에 활용되었다.

GPT는 자연어 처리의 다양한 태스크들, 예를 들면 언어 모델링, 텍스트 분류, 텍스트 생성 등의 태스크를 수행하기 위한 신경망 구조로 되어있다. GPT 모델의 구조는 기본적으로 Transformer 모델을 기반으로 한다.

![](assets/img/2024-01-05-nlp-05/transformer.png){: width="50%" height="50%"}

위 그림은 기본적인 Transformer 구조이다. Transformer에 대한 자세한 내용은 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)를 참고하면 좋다. Transformer는 인코더와 디코더로 구성되어 있다. GPT는 여기서 Transformer 디코더 구조만으로 구성되어 있다.

![](assets/img/2024-01-05-nlp-05/transformer-decoder.png){: width="50%" height="50%"}

위 그림은 Transformer 구조에서 디코더만을 나타낸 것이다. Transformer 구조에서 인코더와 디코더의 가장 큰 구조적인 차이는 `Multi-Head Attention` 메커니즘의 입력이 마스킹이 되어있는지 아닌지로 구분할 수 있을 것이다. 마스킹이 되어있는 경우를 `Masked Multi-Head Attention`이라고 한다. Masked Multi-Head Attention은 토큰 시퀀스의 Self Attention 과정에서 현재 토큰의 시점보다 미래 시점의 토큰들은 마스킹을 하여 미래 시점의 토큰을 예측하도록 학습을 수행하기 위한 구조를 가지고 있다.

GPT에서 사용된 Transformer의 디코더는 인코더 시퀀스에 대한 Attention을 수행할 필요가 없기 때문에 위 그림에서 중간 부분에 있는 Multi-Head Attention 메커니즘 계층을 제거하였다고 한다.

![](assets/img/2024-01-05-nlp-05/gpt-arch.png)

위 그림은 전반적인 GPT의 구조와 입력 방식을 설명하고 있다. 그림에서 확인할 수 있듯 GPT는 Transformer 디코더 구조를 약간 변형하여 활용하고 있으며 수행하려는 NLP 태스크에 따라서 텍스트 입력 방식이 어떻게 변화되는지 알 수 있다.

## GPT와 BERT의 차이
GPT가 먼저 발표된 모델이지만 그 직후 바로 BERT 모델이 발표되면서 두 모델은 필연적으로 비교가 될 수 밖에 없다. 하지만 두 모델은 근본적으로 그 역할이 다르며 그에 따른 구조도 Transformer를 활용한다는 점을 제외하고는 매우 다르다.

우선 GPT는 위에서 설명하였듯 Transformer의 디코더 구조를 기반으로 하고 있다. 따라서 입력 토큰 시퀀스에 대해서 미래 시점은 확인하지 않고 과거 시점들에 대해서만 Attention을 수행하여 현재 시점에 대한 결과물을 출력한다. 반면 BERT는 이름에서 확인할 수 있듯 인코더의 역할을 하는 모델이다. 이에 따라 BERT는 Transformer 인코더 구조를 기반으로 하는 모델 구조를 가지고 있다.

![](assets/img/2024-01-05-nlp-05/transformer-encoder.png){: width="50%" height="50%"}

위 그림은 Transformer의 인코더 구조를 나타내는 그림이다. 인코더는 디코더와 다르게 입력 토큰 시퀀스에서 현재 시점의 토큰에 대해 인코딩을 하는 경우 과거와 미래 시점의 코든 토큰의 시퀀스를 Attention에 활용한다. 즉 양방향으로 정보를 활용하여 현재 시점의 토큰을 인코딩하는 것이다.

## GPT 프레임워크
GPT는 위에서 설명하였듯 Transformer 디코더 기반의 Autoregressive 모델이다. 입력 토큰 시퀀스 및 모델 자신이 생성한 모든 시점의 시퀀스를 다시 모델이 입력받아 그 다음 토큰을 출력하려고 한다. 이런 방식으로 GPT는 사람의 언어 구사를 흉내내도록 학습되어진다.

### 비지도 사전 훈련
사람의 언어 구사를 흉내내도록 학습시키기 위하여 GPT는 사람이 생성한 말뭉치 데이터의 언어 모델링을 수행하도록 학습된다. 주어진 비지도 말뭉치의 토큰들을 다음과 같이 정의하자:

$$
\mathcal{U} = \{ u_1, \cdots, u_n \}.
$$

다음은 GPT의 비지도 사전 훈련에 사용한 표준적인 언어 모델링 목적 함수이다. 우리가 잘 알고 있듯이 Likelihood 함수 형태이다:

$$
L_1(\mathcal{U}) = \sum_i \log p\left( u_i | u_{i - k}, \cdots, u_{i - 1} \right).
$$

여기서 $$k$$는 문맥 윈도우의 크기이다. 즉, 이 언어 모델링은 다음 토큰을 예측할 때 $$k$$개의 이전 토큰들을 참고한다는 의미이다. 여기서 확률 분포 $$p$$는 GPT의 파라미터 $$\boldsymbol{\theta}$$로 파라미터화될 것이다:

$$
L_1(\mathcal{U}) = \sum_i \log p\left( u_i | u_{i - k}, \cdots, u_{i - 1} ; \boldsymbol{\theta} \right).
$$

위에서 설명하였듯 GPT는 다층 Transformer 디코더로 구성되어 언어 모델링을 수행한다. 이 모델은 입력 문맥 토큰들에 Multi-Head Attention을 수행하고 이 결과물을 위치별 완전 연결 순전파 망을 활용하여 타겟 토큰에 대한 출력 분포를 생성한다. 더 정확하게는 아래와 같은 식을 통해서 언어 모델링이 수행된다:

$$
\begin{array}{rl}
\mathbf{h}_0 & = \mathbf{u} \mathbf{W}_e + \mathbf{W}_p. \\
\mathbf{h}_l & = \text{transformer-block}(\mathbf{h}_{l - 1}), \ \forall i \in [1, n]. \\
p(u) & = \text{softmax}\left( \mathbf{h}_n \mathbf{W}_e^\text{T} \right).
\end{array}
$$

여기서 벡터 $$\mathbf{u} = (u_{-k}, \cdots, u_{-1})$$는 토큰들의 문맥 벡터이고 $$n$$은 디코더 계층 개수이다. $$\mathbf{W}_e$$는 토큰 임베딩 행렬이고 $$\mathbf{W}_p$$는 위치 임베딩 행렬이다.

### 지도 파인 튜닝
비지도 사전 훈련 이후에 우리가 원하는 NLP 태스크를 수행하기 위한 지도 파인 튜닝을 진행한다. 지도 파인 튜닝에 대해서 설명하기 위하여 수행하고자 하는 NLP 태스크를 위한 라벨링 데이터셋 $$\mathcal{C}$$를 정의해보자. 이 라벨링 데이터셋의 각 샘플은 입력 토큰의 시퀀스 $$(x^1, \cdots, x^m)$$과 거기에 해당하는 레이블 $$y$$로 구성될 것이다.

입력 시퀀스가 사전 훈련된 모델을 통과하면 마지막 Transformer 블록의 활성 값인 $$\mathbf{h}_n^m$$이 출력된다. 이 벡터는 파라미터가 $$\mathbf{W}_y$$인 추가적인 선형 계층에 통과되어 레이블 y를 예측한다:

$$
p(y | x^1, \cdots, x^m) = \text{softmax} \left( \mathbf{h}_n^m \mathbf{W}_y \right).
$$

이를 통해서 다음과 같은 목적 함수를 획득할 수 있다:

$$
L_2(\mathcal{C}) = \sum_{\mathbf{x}, y} \log p(y | x^1, \cdots, x^m).
$$

추가적으로 저자들은 지도 파인 튜닝 과정에서 추가적인 보조 목적 함수를 통해서 지도 모델의 일반화 능력을 향상시키고, 수렴을 더 빠르게 시켜준다는 것을 주장한다. 이는 비지도 사전 훈련의 목적 함수를 보조 목적 함수로 추가함으로써 가능하다:

$$
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C}).
$$

## GPT의 발전 과정

![](assets/img/2024-01-05-nlp-05/gpt-road-map.png)

GPT 모델은 여러 변화를 거치면서 발전되어 왔고, 그 과정에서 다양한 NLP 기술들의 발전에 크게 기여하였다. GPT 모델 이전에는 다양한 종류의 NLP 태스크 각각에 대한 대량의 라벨링 데이터가 필요하였다. 즉, 모든 NLP 모델은 태스크 하나에만 특화되어 왔었으며 따라서 자신이 훈련된 데이터셋을 넘어서는 태스크는 절대로 수행할 수 없었다. 또한 이는 전 세계에 존재하는 수많은 텍스트 데이터가 라벨링이 되어있지 않다는 이유로 활용할 수 없었다는 문제도 있었다.

### GPT-1
이러한 제한 조건들을 타파하기 위하여 OpenAI에서는 `GPT-1`이라는 이름의 생성형 AI 모델을 제안했다. 이 모델은 라벨링이 되어있지 않은 다량의 말뭉치 데이터를 활용하여 사전 훈련을 할 수 있었고 다양한 NLP 하위 태스크, 예를 들면 감정 분석, 분류, 질의 응답 등의 태스크를 수행하기 위해서는 상대적으로 적은 양의 훈련용 지도 학습 데이터만을 요구하였다.

GPT-1은 최초로 텍스트를 읽고 질문에 답변을 할 수 있는 모델로 인정되어 왔다. OpenAI는 GPT-1을 2018년에 출시했는데 기존의 AI 접근들과는 다르게 텍스트 기반의 데이터를 좀 더 자연스러운 방식으로 컴퓨터가 이해할 수 있도록 하였다는 점에서 AI 발전에 귀중한 한 발짝으로 평가받는다.

GPT-1을 사전 훈련시키기 위하여 거대한 말뭉치 데이터셋인 `BooksCorpus` 데이터셋이 활용되었다. GPT-1은 이를 학습시키기 위하여 12 계층의 Transformer 디코더 구조를 차용하였다. 이러한 사전 훈련의 매우 성공적인 결과로는 GPT-1이 다양한 서로 다른 태스크들에 대해서 Zero-Shot 성능을 보여줬다는 점이다. 이 결과는 생성 언어 모델링이 훌륭한 사전 훈련 아이디어와 결합하면 태스크 특화가 아닌 일반적인 AI의 모습을 보여줄 수 있다는 점을 증명했다.

### GPT-2
2019년에는 OpenAI가 더 향상된 언어 모델 개발을 위하여 `GPT-2`를 개발하였다. GPT-2는 GPT-1에 비해서 훨씬 더 큰 데이터셋과 파라미터 수를 사용한 모델이다. GPT-2는 GPT-1에 비하여 약 10배 정도의 파라미터 수인 15억 개의 파라미터를 보유하고 있다. (GPT-1의 파라미터 수는 약 1억 1700만 개이다.) 또한 GPT-2는 학습을 위하여 GPT-1에 비해서 약 10배 더 큰 데이터셋을 활용하였다고 한다. 이러한 방식으로 사전 훈련된 GPT-2 모델은 다른 추가적인 지도 파인 튜닝이 없이도 번역, 요약 등의 NLP 태스크를 훌륭하게 수행하였다고 한다.

GPT-2를 사전 훈련시키기 위하여 활용한 데이터셋은 다양한 분야와 문맥에서 추출된 자연어 데이터로 구성되었다. 이를 위하여 `Common Crawl`과 같은 웹 스크랩를 활용하는 방법이 있다. 이러한 웹 스크랩은 다만 현재의 언어 모델링 데이터셋들에 비해서 매우 거대함에도 불구하고 퀄리티 문제에서 자유롭지 못하였다.

대신 OpenAI에서는 문서 퀄리티를 향상시킬 수 있는 새로운 웹 스크랩을 제작했다. 이 웹 스크랩을 제작하기 위하여 사람에 의하여 선별되고 필터링된 웹 페이지만을 수집하였다고 한다. 이러한 선별 및 필터 작업을 직접 수동으로 수행하는 것은 그 비용이 너무 많이 들기 때문에 저자들은 Karma를 3개 이상 받은 레딧 문서들을 우선적으로 선별하였다고 한다. Karma를 3개 이상 받았다는 것은 재미있고 교육적이라는 좋은 지표로 볼 수 있다.

이렇게 제작된 데이터셋의 이름은 `WebText`라고 한다. 이 데이터셋은 약 4500만 개의 링크로부터 추출되었다. HTML 문서로부터 텍스트를 추출하기 위하여 `Dragnet`과 `Newspaper Content Extractor`를 조합하여 활용하였다고 한다. 이렇게 추출된 문서들은 2017년 12월 이후 내용은 포함하지 않고 있다. 수집된 이후 몇 가지 복제된 것들을 제거하고 클리닝하는 작업을 거친 이후 완료된 문서는 800만 개의 문서에 40GB의 텍스트 용량을 보유하고 있다.

저자들은 WebText 데이터셋을 제작하는 과정에서 위키피디아가 출처인 데이터는 모두 제거했다고 한다. 그 이유는 다른 데이터셋과 겹치는 내용들이 많이 있을 것으로 생각되고 이렇게 겹치는 내용들이 훈련용 데이터셋과 테스트용 데이터셋에서 동시에 등장하면 좋은 평가가 이루어지지 않기 때문이라고 한다.

### GPT-3
OpenAI는 이후 2020년에 `GPT-3`를 발표하였다. GPT-3는 매우 거대한 언어 예측 및 생성 모델이며 이제는 단순 텍스트 생성 정도가 아니라 매우 긴 구문까지 생성할 수 있게 되었다. (문맥 윈도우의 크기가 2049, 4096 토큰에 이른다.) OpenAI는 GPT-3를 클라우드 기반의 API 형태로 제한적인 접근만이 가능하도록 서비스를 제공하고 있다.

GPT-3는 사람과 구분하기 힘들 정도로 자연어 생성을 자연스럽게 하며 추가적으로 지금까지는 볼 수 없었던 태스크들도 수행 가능했다. 기본적인 수식 처리, 코드 구현 등의 태스크들이 그것이다. 결과적으로 GPT-3는 NLP 태스크 뿐 아니라 다양한 비즈니스 활동을 신속하고 정확하게 보조할 수 있게 되었다.

GPT-3는 GPT-2에 비해서 약 100배 정도 많은 수의 파라미터인 약 1750억 개의 파라미터 수를 보유하고 있다. 또한 이를 학습시키기 위해서 5000억 개의 단어가 포함된 말뭉치를 사용하였다고 한다. 이렇게 거대한 모델이기에 추론 비용이 막대하였고 실질적으로 가볍게 활용하기는 어려웠다. 이러한 복잡함 및 거대함 덕분에 많은 사람들은 GPT-3를 `궁극적인 블랙박스 AI 방법론`이라고들 말한다.

GPT-3를 학습시키기 위해서 사용된 말뭉치를 제작하기 위하여 OpenAI는 웹 스크랩 Common Crawl을 활용하여 여러 필터링 및 정제 작업을 거쳤다. 다음은 그 정제 작업을 정리한 것이다:

- Common Crawl을 다운로드하여 고퀄리티의 레퍼런스 범주인 말뭉치와의 유사도를 기반으로 필터링을 하였다.
- 문서 단위에서 `Fuzzy Deduplication` 방법을 통해서 중복성을 제거하였고 이를 통해 검증용 셋의 정합성을 보존하였다.
- 잘 알려진 고퀄리티 레퍼런스 말뭉치를 훈련 과정에서 추가하여 기존 데이터셋의 다양성을 높였다.

특히 3번째 정제 작업에서 포함되는 고퀄리티 레퍼런스는 확장된 WebText 데이터셋, 2개의 인터넷 기반 도서(Books1, Books2), 그리고 영문 위키피디아 등이다.

![](assets/img/2024-01-05-nlp-05/gpt-3-task.png){: width="70%" height="70%"}

위 그림은 기존의 파인 튜닝 방식의 거대 언어 모델 학습 방법에 비해서 GPT-3가 `Zero-Shot`, `One-Shot`, `Few-Shot`으로 어떻게 NLP 태스크를 수행하는지 설명하는 그림이다. GPT-3는 이러한 방식으로 추가 학습 없이 프롬프트를 통한 태스크 설명만으로도 사람 수준의 훌륭한 성능을 보여줄 수 있다.

### GPT-3.5 & GPT-4
OpenAI는 2022년에 GPT-3를 개선한 `GPT-3.5`를 발표한다. GPT-3.5는 더욱 개선된 말뭉치를 통해서 사전 훈련이 되었다고 한다. 이 말뭉치는 텍스트 뿐 아니라 다양한 코드와 텍스트의 조합으로 구성되어 있다고 한다. 이 말뭉치를 구성하기 위하여 웹에서 수집된 다양한 문서들, 수 만개의 위키피디아 문서들, 미디어 포스트, 뉴스 등을 수집하였다고 한다. 이를 통해서 GPT-3.5는 단어, 문장, 그리고 다양한 웹 컴포넌트 사이의 관계들을 학습할 수 있었다고 한다. GPT-3.5는 `ChatGPT`에 탑재되어 우리가 활용할 수 있도록 하였다.

그리고 가장 최신 GPT 버전은 2023년 3월 14일에 발표한 `GPT-4`이다. GPT-4는 기본적으로 이전 GPT 모델들과는 다르게 `멀티모달 거대 언어 모델`이다. 즉, 텍스트 뿐 아니라 이미지 등의 텍스트가 아닌 데이터도 이해할 수 있다는 것이다. GPT-4는 `ChatGPT Plus`에 탑재되어 있다.

공공 데이터 및 써드파티 공급자로 부터 라이센싱 받은 데이터들을 활용하였을 뿐 아니라 GPT-4는 OpenAI의 정책을 준수하기 위하여 인간과 AI의 입력을 기반으로 하는 강화 학습을 활용하여 학습이 되었다. 또한 GPT-4는 GPT-3에 비해서 문맥 윈도우의 크기가 8192, 32868 토큰까지 늘어났다. 아래 그림은 GPT-4를 학습시키기 위하여 강화 학습이 어떻게 활용되었는지를 설명하는 그림이다.

![](assets/img/2024-01-05-nlp-05/rlhf.png)

## 정리
GPT는 그동안의 지도 학습 기반의 언어 모델 연구 방법에서 거대 말뭉치에 대한 사전 훈련과 태스크 특화된 지도 파인 튜닝으로 그 패러다임을 전환한 연구 결과로 평가받는다. 또한 간단하다면 간단한 언어 모델링 방식만 가지고도 충분히 인간 수준으로 언어를 이해할 수 있고 그에 맞는 태스크를 수행할 수 있다는 것을 증명한 연구이기도 하다. 이후 등장한 대부분의 거대 언어 모델 연구가 생성형 위주로 이뤄지고 있다는 점이 바로 그 증거일 것이다.

하지만 GPT에도 여러 문제점이 있다. 학습에 활용한 거대 말뭉치에 어떤 문제되는 발언들이 포함되어 있는지를 우리가 전부 캐치하기 어렵다는 점 때문에 GPT가 문제되는 발언들을 생성할 수 있다는 점이다. 비도덕적인 발언이나 또는 흔히 `환상`(Hallucination)이라고 불리우는 거짓된 정보를 그럴듯하게 생성하는 점들일 것이다. 이러한 문제들을 해결하려는 다양한 시도들이 있다. 이러한 방법들 역시 향후에 또 살펴보도록 할 것이다.

또한 GPT의 또 다른 한계점은 바로 실제 언어 기반의 추론을 한다고 볼 수 없다는 점이다. GPT를 학습시키기 위해서는 말뭉치의 Log-Likelihood를 최대화하는 방식이 사용되었다. 이는 통계적으로 그럴듯한 문장만을 `외워서` 생성하고 있다고 봐야 한다. 또한 GPT는 기억력을 가지지 못하다는 한계점 역시 있다. 물론 GPT에 기억력을 부여하려는 시도는 이미 많이 진행되고 있으며 추후 다시 알아보도록 할 것이다.

## 참고 자료
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=transaction)
- [Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions](https://arxiv.org/abs/2305.10435)

## 수정 사항
- 2024.01.16
    - 최초 게제