---
layout: post
use_math: true
title: "[Natural Language Processing] 3. Transformer"
date: 2023-02-25 00:00:00
tagline: "자연어 처리에서 최근 가장 각광받는 신경망 모델인 Transformer에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-02-25-nlp-03
---

이번 포스트에서는 최근 자연어 처리 분야에서 각광받는 대용량 언어 모델들의 기본 구성 요소가 되는 신경망 모델인 `Transformer`에 대해서 다룬다. 먼저 Transformer이 처음 나오게 된 계기인 기계 번역 태스크에 대해서 정리하고 어떤 흐름을 통해서 Transformer이 등장하게 되었는지까지 다뤄볼 계획이다.

## 기계 번역과 Sequence-to-Sequence 모델
이전 [포스트](https://hcnoh.github.io/2023-02-17-nlp-01)에서 기계 번역에 대해서 간단히 다뤘었다. 기계 번역은 특정 언어로 주어진 문장을 목표 언어로 번역하는 태스크를 말한다. 예를 들면 한국어를 영어로 번역하는 기계 번역 태스크가 있을 수 있겠다. 기존 딥 러닝 기반의 기계 번역은 보통 입력 문장을 `인코더`(Encoder)를 통해서 인코딩을 수행하고 이 인코딩된 표현을 바탕으로 `디코더`(Decoder)가 목표 언어로 다시 재생성하는 방식을 채택하였다. 이를 가장 잘 수행했던 모델은 바로 RNN 기반의 `Sequence-to-Sequence`(seq2seq) 모델이다.

![](assets/img/2023-02-25-nlp-03/2023-02-25-06-41-06.png)

위의 그림은 영어 문장을 한국어 문장으로 번역하는 seq2seq 번역 모델의 예시를 나타낸 그림이다. 먼저 주어진 입력 문장은 RNN 기반의 인코더 모델을 통해 인코딩이 수행된다. 보통 인코딩은 마지막 \<EOS\> 토큰의 입력에 대한 인코더의 은닉 표현(Hidden Representation)을 인코딩의 결과 표현으로 활용한다. 이는 디코더 RNN 셀의 초기 은닉 상태(Hidden State) 벡터로 활용되어 디코딩 과정에서 문장 재구축에 활용되게 된다.

## Sequence-to-Sequence 모델의 문제점
일반적으로 심층 신경망의 계층이 많아질수록 훈련 과정에서 뒤쪽 계층에 비해서 앞쪽 계층의 가중치 변화가 점점 적어지면서 앞쪽 계층의 가중치가 훈련이 잘 되지 않는 문제점이 발생하곤 한다. 이를 `Vanishing Gradient` 문제라고 하는데 이 원인은 심층 신경망의 계층 사이사이의 활성 함수(Activation Function)로 인하여 가중치의 Gradient의 절대값이 앞쪽 계층으로 갈 수록 감쇄하기 때문이다. 활성 함수로 많이 쓰이는 Sigmoid 함수의 Gradient 값은 항상 0과 1 사이의 값을 갖고 이 값이 자꾸 곱해지는 것이 반복될수록 Gradient 값 역시 감쇄가 일어날 것이다. 이러한 문제를 해결하기 위하여 Sigmoid 함수 대신 ReLU 같은 함수들을 활용하는 방향으로 연구가 되어왔다.

이는 기계 번역같은 태스크에서 쓰이는 RNN 기반의 모델에서도 많이 발생하는 문제이다. 입력 시퀀스는 시간 단계를 따라가며 반복적으로 RNN 셀에 입력이 들어가고 이는 최근 입력값에 대해서보다 이전 입력값에 대한 Gradient 절대값이 매우 작게 되는 현상들을 발생시킬 것이다. 특히 RNN 셀의 내부에서 활성 함수는 Sigmoid와 그 특징이 유사한 Tanh 함수를 사용하기 때문에 이 현상은 더욱 심할 것이다.

즉 입력 시퀀스의 길이가 길어질수록 현재 입력값에 비해서 이전 입력값에 대한 훈련 정보는 점차 사라진다는 것이고 이는 모델이 시퀀스를 바라볼때 최근 입력값에만 집중하는 현상이 벌어진다. 이는 기계 번역 뿐 아니라 문맥을 활용해야하는 다양한 자연어 처리 태스크에서 그 문제를 발생시키곤 했다.

## Attention 메커니즘

## Transformer

### Query, Key, Value

### Self-Attention

### Multihead Attention

## 참고 자료
- 

## 수정 사항
- 2023.02.25
    - 최초 게제