---
layout: post
use_math: true
title: "[Natural Language Processing] 3. Transformer"
date: 2023-02-25 00:00:00
tagline: "자연어 처리에서 최근 가장 각광받는 신경망 모델인 Transformer에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-02-25-nlp-03
---

이번 포스트에서는 최근 자연어 처리 분야에서 각광받는 대용량 언어 모델들의 기본 구성 요소가 되는 신경망 모델인 `Transformer`에 대해서 다룬다. 먼저 Transformer이 처음 나오게 된 계기인 기계 번역 태스크에 대해서 정리하고 어떤 흐름을 통해서 Transformer이 등장하게 되었는지까지 다뤄볼 계획이다.

## 기계 번역과 Sequence-to-Sequence 모델
이전 [포스트](https://hcnoh.github.io/2023-02-17-nlp-01)에서 기계 번역에 대해서 간단히 다뤘었다. 기계 번역은 특정 언어로 주어진 문장을 목표 언어로 번역하는 태스크를 말한다. 예를 들면 한국어를 영어로 번역하는 기계 번역 태스크가 있을 수 있겠다. 기존 딥 러닝 기반의 기계 번역은 보통 입력 문장을 `인코더`(Encoder)를 통해서 인코딩을 수행하고 이 인코딩된 표현을 바탕으로 `디코더`(Decoder)가 목표 언어로 다시 재생성하는 방식을 채택하였다. 이를 가장 잘 수행했던 모델은 바로 RNN 기반의 `Sequence-to-Sequence`(seq2seq) 모델이다.

![](assets/img/2023-02-25-nlp-03/2023-02-25-06-41-06.png)

위의 그림은 영어 문장을 한국어 문장으로 번역하는 seq2seq 번역 모델의 예시를 나타낸 그림이다. 먼저 주어진 입력 문장은 RNN 기반의 인코더 모델을 통해 인코딩이 수행된다. 보통 인코딩은 마지막 \<EOS\> 토큰의 입력에 대한 인코더의 은닉 표현(Hidden Representation)을 인코딩의 결과 표현으로 활용한다. 이는 디코더 RNN 셀의 초기 은닉 상태(Hidden State) 벡터로 활용되어 디코딩 과정에서 문장 재구축에 활용되게 된다.

## Sequence-to-Sequence 모델의 문제점
일반적으로 심층 신경망의 계층이 많아질수록 훈련 과정에서 뒤쪽 계층에 비해서 앞쪽 계층의 가중치 변화가 점점 적어지면서 앞쪽 계층의 가중치가 훈련이 잘 되지 않는 문제점이 발생하곤 한다. 이를 `Vanishing Gradient` 문제라고 하는데 이 원인은 심층 신경망의 계층 사이사이의 활성 함수(Activation Function)로 인하여 가중치의 Gradient의 절대값이 앞쪽 계층으로 갈 수록 감쇄하기 때문이다. 활성 함수로 많이 쓰이는 Sigmoid 함수의 Gradient 값은 항상 0과 1 사이의 값을 갖고 이 값이 자꾸 곱해지는 것이 반복될수록 Gradient 값 역시 감쇄가 일어날 것이다. 이러한 문제를 해결하기 위하여 Sigmoid 함수 대신 ReLU 같은 함수들을 활용하는 방향으로 연구가 되어왔다.

이는 기계 번역같은 태스크에서 쓰이는 RNN 기반의 모델에서도 많이 발생하는 문제이다. 입력 시퀀스는 시간 단계를 따라가며 반복적으로 RNN 셀에 입력이 들어가고 이는 최근 입력값에 대해서보다 이전 입력값에 대한 Gradient 절대값이 매우 작게 되는 현상들을 발생시킬 것이다. 특히 RNN 셀의 내부에서 활성 함수는 Sigmoid와 그 특징이 유사한 Tanh 함수를 사용하기 때문에 이 현상은 더욱 심할 것이다. 이 문제를 `Long Term Dependency`라고도 한다.

즉 입력 시퀀스의 길이가 길어질수록 긴 입력 시퀀스에 대해서 고정된 크기의 은닉 상태 벡터로 정보를 입력함에 있어서 정보의 손실이 발생하고 이는 너무 긴 길이의 의존성, 다시 말해 Long Term Dependency를 적절히 인코딩하기 어려워  현재 입력값에 비해서 이전 입력값에 대한 훈련 정보는 점차 사라진다는 결과로 나타난다. 결국 이로 인하여 모델이 시퀀스를 바라볼때 최근 입력값에만 집중하는 현상이 벌어진다. 이는 기계 번역 뿐 아니라 문맥을 활용해야하는 다양한 자연어 처리 태스크에서 그 문제를 발생시키곤 했다.

## Attention 메커니즘
이러한 seq2seq 모델의 문제점을 해결하기 위해서 등장한 것이 바로 `Attention 메커니즘`이다. 기존 seq2seq 모델의 단점이었던 너무 멀리 떨어진 입력값에 대한 영향력이 너무 낮아진다는 문제점을 해결하기 위해서 멀리 떨어진 입력값에도 충분한 가중치를 부여하여 디코딩 과정에서 활용하겠다는 의도가 반영된 것이 바로 이 Attention 메커니즘이다. Attention 메커니즘은 본 블로그의 포스트들([[Attention] Bahdanau Attention 개념 정리](https://hcnoh.github.io/2018-12-11-bahdanau-attention), [[Attention] Luong Attention 개념 정리](https://hcnoh.github.io/2019-01-01-luong-attention))을 참고하면 더 자세히 확인할 수 있다.

![](assets/img/2023-02-17-nlp-01/2023-02-20-15-38-47.png)

심층 신경망 모델은 기본적으로 주어진 특징을 적절한 은닉 벡터로 인코딩하는 과정을 수행하려고 한다. 이 과정에서 더 중요한 특징들을 스스로 찾아서 집중하는 현상이 일어나는데 Attention 메커니즘은 이를 좀 더 수월하게끔 구조적으로 설계하는 방법론이다. Long Term Dependecy를 반영하기에는 Vanishing Gradient 문제가 발생하기에 모델이 스스로 가중치를 선택하여 Vanishing Gradient 문제가 발생하는 긴 경로를 생략하고 직접적으로 멀리 떨어진 입력값에 집중할 수 있도록 도와주는 것이 Attention 메커니즘이다.

Attention 메커니즘은 기본적으로 `Attention 점수`를 계산하는 것으로부터 출발한다. 이는 일반적으로 현재 주어진 디코더의 입력값에 대한 은닉 벡터와 입력값 전체에 대한 은닉 벡터들 사이의 내적을 통해서 계산한다. 위의 그림에서 인코더 입력 토큰들 "I", "Love", "You", "\<EOS\>" 토큰들에 대한 인코더 은닉 벡터를 각각 $$\mathbf{h}_1, \mathbf{h}_2, \mathbf{h}_3, \mathbf{h}_4$$라고 하자. 또한 디코더 입력 토큰들인 "\<BOS\>", "나는", "너를", "사랑해" 각각에 대한 디코더 은닉 벡터를 각각 $$\mathbf{s}_1, \mathbf{s}_2, \mathbf{s}_3, \mathbf{s}_4$$라고 하자. 여기서 디코더 입력 토큰 "나는"에 대한 인코더 입력 토큰 "You"의 Attention 점수는 다음과 같다:

$$
a_{2}^{(3)} = \frac{\exp\left( \mathbf{s}_2^\text{T} \mathbf{h}_3 \right)}{\sum_{j=1}^4 \exp \left( \mathbf{s}_2^\text{T}\mathbf{h}_j \right) }.
$$

여기서 디코더 입력 토큰 "나는"에 대한 모든 Attention 점수의 벡터화인 `Alignment 벡터`는 다음과 같이 정의한다:

$$
\mathbf{a}_2 = \left[ a_2^{(1)}, a_2^{(2)}, a_2^{(3)}, a_2^{(4)} \right] = \text{Softmax} \left( \left( \mathbf{s}_2^\text{T} \mathbf{h}_j \right)_{j=1}^4 \right) \in \mathbb{R}^4.
$$

여기서는 입력 토큰 "나는"에 대한 Attention 점수를 뽑은 것이기에 아마도 인코더 입력 토큰 "I"의 Attention 점수가 가장 높을 것이라고 추측할 수 있다. 좌우지간 Alignment 벡터는 인코더의 입력 토큰 시퀀스의 길이가 $$L$$인 경우에 디코더의 $$t$$번째 입력 토큰에 대해서 다음과 같이 일반적으로 정의할 수 있다:

$$
\mathbf{a}_t = \left[ a_t^{(1)}, \cdots a_t^{(L)} \right] = \text{Softmax} \left( \left( \mathbf{s}_t^\text{T} \mathbf{h}_j \right)_{j=1}^L \right) \in \mathbb{R}^L.
$$

이렇게 정의된 Alignment 벡터 $$\mathbf{a}_t$$는 디코더 입력의 $$t$$단계에서의 토큰에 대한 인코더 입력 토큰들 전체에 대해 Attention 점수를 나타내는 벡터라고 보면 된다. 모두들 알고 있듯이 $$\mathbf{a}_t$$는 정상화(Normalization)되어있기 때문에 모든 성분의 합이 1인 것을 확인할 수 있다. 이렇게 구해진 Attention 점수들은 인코더 은닉 벡터들인 $$\left\{ \mathbf{h}_j \right\}_{j=1}^L$$와 가중 합계로 계산되어서 `문맥 벡터`(Context 벡터)라고 불리우는 표현으로 변환된다:

$$
\mathbf{c}_t = \sum_{j=1}^L \mathbf{s}_t^\text{T}\mathbf{h}_j.
$$

이 문맥 벡터는 디코더가 현재 단계 $$t$$에서의 디코더 출력을 생성하는 떄에 사용되게 된다. 즉 이를 통해서 모델은 문맥 벡터로 활용될 인코더 입력 토큰들에 대한 정보를 어느 정도 수준으로 선택할지 가중치를 스스로 부여할 수 있도록 학습이 이루어지게 되며 결과적으로 매우 긴 입력 시퀀스에 대해서도 이전보다는 높은 수준으로 Long Term Dependency를 잘 반영하여 정보를 인코딩할 수 있다.

## Transformer
`Transformer`는 [Attention Is All You Need](https://arxiv.org/abs/1706.03762)라는 논문에서 처음으로 제안한 Attention 기반의 심층 신경망 모델이다. 기존의 RNN 또는 CNN 기반의 모델들에서 탈피하여 오직 Attention 메커니즘만을 기반으로 하는 새로운 모델이다. Transformer는 등장하자마자 모든 언어 모델들의 기반이 되는 역할을 하였고 단순 자연어 처리에서 뿐 아니라 시계열 데이터를 다루는 음성 영역, 더 나아가 영상 영역까지도 활용되는 매우 중요한 모델이 되었다.

### 인코더-디코더 스택
대부분의 심층 신경망 기반의 시퀀스 변환기는 seq2seq 모델의 형태를 가지고 있으며 이는 구조적으로 `인코더-디코더` 구조를 따른다. 이와 마찬가지로 Transformer도 인코더-디코더 구조를 따른다. Transformer는 기존 인코더-디코더 구조와는 다르게 `Self-Attention` 기반의 스택 구조의 인코더-디코더 구조를 갖고 있다.

![](assets/img/2023-02-25-nlp-03/2023-02-28-21-17-44.png){: width="50%" height="50%"}

위의 그림은 논문에서 소개하는 Transformer 모델의 구조를 나타내는 그림이다. `인코더`는 기본적으로 여러개의 동일한 계층을 쌓은 형태의 스택으로 구성되어 있다. 각각의 계층은 두 개의 보조 계층(Sub-Layer)로 구성되어 있다. 첫 번째 보조 계층은 `Multi-Head Self-Attention` 메커니즘이고 두 번째 보조 계층은 간단한 위치별 완전 연결 순전파 망(Position-Wise Fully Connected Feed-Forward Network)이다. 완전 연결 순전파 망이 위치별로 구성되어있다는 의미는 위치별, 즉 토큰별 은닉 벡터들을 동일한 가중치를 통해 투영하는 계층이 존재한다는 의미이다.

보조 계층 각각의 출력에는 `Residual Connection`이라는 연결과 `Layer Normalizataion`이라는 정상화 방법론이 적용되었다. Residual Connection은 입력과 출력을 한 번 더하는 방식으로 출력 연산을 추가하여 Vanishing Gradient를 줄이는 방법론이다. Layer Normalization은 Batch Normalization과 유사한 방법론이며 나중에 한 번 다뤄보도록 하겠다. Batch Normalization에 대해서는 다음의 [포스트](https://hcnoh.github.io/2018-11-27-batch-normalization)를 참고하면 된다.

### Attention
Transformer의 `Attention` 함수는 기본적으로 벡터 형태인 `Query`, `Key`, `Value`를 바탕으로 주어진 Query와 Key-Value 쌍으로부터 출력으로 매핑하는 함수라고 생각할 수 있다. 주어진 Query에 대해서 모든 Key 값들과 Attention 점수를 계산하고 이를 가중치로 하여 각각의 대응하는 Key 값에 할당한다. 이러한 방식의 Attention 메커니즘을 저자들은 `Scale Dot-Product Attention`이라고 부른다.

![](assets/img/2023-02-25-nlp-03/2023-02-28-23-43-41.png)

위 그림의 왼쪽은 Scaled Dot-Product Attention 메커니즘을 설명하는 그림이다. 입력은 $$d_k$$차원의 벡터들로 구성된 하나의 Query와 여러 Key들의 집합, 그리고 $$d_v$$차원의 벡터들로 구성된 여러 Value들의 집합이다. Query는 모든 Key들과 내적(Dot-Product)를 수행하고 그 결과물을 $$\sqrt{d_k}$$로 나눈 다음 Softmax 함수를 적용하여 가중치를 계산한다. 이는 행렬 연산을 통해서 좀 더 간단히 쓸 수 있다.

Query는 $$\mathbf{Q} \in \mathbb{R}^{L_q \times d_k}$$, Key 집합은 $$\mathbf{K} \in \mathbb{R}^{L_k \times d_k}$$, 그리고 Value 집합은 $$\mathbf{V} \in \mathbb{R}^{L_k \times d_v}$$라고 행렬 형태로 쓸 수 있다. 이 경우 Attention 함수는 다음과 같이 쓸 수 있다:

$$
\text{Attention}\left( \mathbf{Q}, \mathbf{K}, \mathbf{V} \right) = \text{Softmax} \left(  \frac{\mathbf{Q}\mathbf{K}^\text{T}}{\sqrt{d_k}} \right) \mathbf{V}.
$$

이러한 방식으로 Query, Key, Value에 대한 하나의 Attention 함수를 적용하는 대신 이를 $$h$$번 반복 수행하여 획득한 결과를 활용하는 `Multi-Head Attention`이 더욱 효과가 좋다는 것을 저자들은 주장한다. Multi-Head Attention이란 Query, Key, Value 각각을 $$h$$번에 걸쳐서 선형 변환을 통해 학습된 새로운 공간으로 투영하고(여기서 Query, Key, Value 각각을 투영한 공간의 차원은 $$d_k$$, $$d_k$$, $$d_v$$이다.) 이 $$h$$개의 투영된 벡터들을 통해서 Attention을 $$h$$번 수행는 것을 말한다. 즉, Attention Head가 여러개(여기서는 $$h$$개)가 있고 각각의 Head가 서로 다른 표현 보조 공간(Representation Subspace)로부터 서로 다른 Attention 정보를 추출하는 것을 동시에 학습할 수 있도록 하는 구조이다.

Multi-Head Attention은 모델로 하여금 Head가 서로 다른 Attention 정보를 획득할 수 있도록 하고 Single-Head의 경우에는 이 서로 다른 Attention 정보를 따로 획득하는 것이 아니라 이들을 모두 평균치를 내서 다루게 된다. Multi-Head Attention을 수식으로 설명하면 다음과 같다:

$$
\begin{array}{rl}
\text{MultiHead} \left( \mathbf{Q}, \mathbf{K}, \mathbf{V} \right) & = \text{Concat} \left( \text{head}_1, \text{head}_2, \cdots, \text{head}_h \right) \mathbf{W}^O, \\
\text{where} \ \text{head}_i & = \text{Attention} \left( \mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V \right).
\end{array}
$$

여기서

$$
\mathbf{Q} \in \mathbb{R}^{L_q \times d_\text{model}}, \mathbf{K} \in \mathbb{R}^{L_k \times d_\text{model}}, \mathbf{V} \in \mathbb{R}^{L_k \times d_\text{model}}
$$

이고,

$$
\mathbf{W}_i^Q \in \mathbb{R}^{d_\text{model} \times d_k}, \mathbf{W}_i^K \in \mathbb{R}^{d_\text{model} \times d_k}, \mathbf{W}_i^V \in \mathbb{R}^{d_\text{model} \times d_v}, \mathbf{W}^O \in \mathbb{R}^{ d_v \times d_\text{model}}
$$

은 투영을 위한 매개변수이다.

### 위치별 완전 연결 순전파 망
Attention 보조 계층에 추가적으로 각각의 인코더와 디코더는 완전 연결 순전파 망을 보유하고 있다. 위에서 설명하였듯이 각각의 완전 연결 순전파 망은 동일한 가중치를 통해서 각 토큰의 위치별로 독립적이고 동일하게 적용된다. 이러한 이유로 위치별 완전 연결 순전파 망이라고 부르게 된다. 이 망은 두 개의 선형 변환과 그 사이에 ReLU 활성 함수로 구성되어 있다:

$$
\text{FFN} = \max \left( 0, \mathbf{x} \mathbf{W}_1 + \mathbf{b}_1 \right) \mathbf{W}_2 + \mathbf{b}_2.
$$

## 참고 자료
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## 수정 사항
- 2023.02.25
    - 최초 게제
- 2023.02.27
    - Attention 메커니즘 내용 정리