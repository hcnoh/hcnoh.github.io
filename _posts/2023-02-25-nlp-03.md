---
layout: post
use_math: true
title: "[Natural Language Processing] 3. Transformer"
date: 2023-02-25 00:00:00
tagline: "자연어 처리에서 최근 가장 각광받는 신경망 모델인 Transformer에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-02-25-nlp-03
---

이번 포스트에서는 최근 자연어 처리 분야에서 각광받는 대용량 언어 모델들의 기본 구성 요소가 되는 신경망 모델인 `Transformer`에 대해서 다룬다. 먼저 Transformer이 처음 나오게 된 계기인 기계 번역 태스크에 대해서 정리하고 어떤 흐름을 통해서 Transformer이 등장하게 되었는지까지 다뤄볼 계획이다.

## 기계 번역과 Sequence-to-Sequence 모델
이전 [포스트](https://hcnoh.github.io/2023-02-17-nlp-01)에서 기계 번역에 대해서 간단히 다뤘었다. 기계 번역은 특정 언어로 주어진 문장을 목표 언어로 번역하는 태스크를 말한다. 예를 들면 한국어를 영어로 번역하는 기계 번역 태스크가 있을 수 있겠다. 기존 딥 러닝 기반의 기계 번역은 보통 입력 문장을 `인코더`(Encoder)를 통해서 인코딩을 수행하고 이 인코딩된 표현을 바탕으로 `디코더`(Decoder)가 목표 언어로 다시 재생성하는 방식을 채택하였다. 이를 가장 잘 수행했던 모델은 바로 RNN 기반의 `Sequence-to-Sequence`(seq2seq) 모델이다.

![](assets/img/2023-02-25-nlp-03/2023-02-25-06-41-06.png)

위의 그림은 영어 문장을 한국어 문장으로 번역하는 seq2seq 번역 모델의 예시를 나타낸 그림이다. 먼저 주어진 입력 문장은 RNN 기반의 인코더 모델을 통해 인코딩이 수행된다. 보통 인코딩은 마지막 \<EOS\> 토큰의 입력에 대한 인코더의 은닉 표현(Hidden Representation)을 인코딩의 결과 표현으로 활용한다. 이는 디코더 RNN 셀의 초기 은닉 상태(Hidden State) 벡터로 활용되어 디코딩 과정에서 문장 재구축에 활용되게 된다.

## Sequence-to-Sequence 모델의 문제점
일반적으로 심층 신경망의 계층이 많아질수록 훈련 과정에서 뒤쪽 계층에 비해서 앞쪽 계층의 가중치 변화가 점점 적어지면서 앞쪽 계층의 가중치가 훈련이 잘 되지 않는 문제점이 발생하곤 한다. 이를 `Vanishing Gradient` 문제라고 하는데 이 원인은 심층 신경망의 계층 사이사이의 활성 함수(Activation Function)로 인하여 가중치의 Gradient의 절대값이 앞쪽 계층으로 갈 수록 감쇄하기 때문이다. 활성 함수로 많이 쓰이는 Sigmoid 함수의 Gradient 값은 항상 0과 1 사이의 값을 갖고 이 값이 자꾸 곱해지는 것이 반복될수록 Gradient 값 역시 감쇄가 일어날 것이다. 이러한 문제를 해결하기 위하여 Sigmoid 함수 대신 ReLU 같은 함수들을 활용하는 방향으로 연구가 되어왔다.

이는 기계 번역같은 태스크에서 쓰이는 RNN 기반의 모델에서도 많이 발생하는 문제이다. 입력 시퀀스는 시간 단계를 따라가며 반복적으로 RNN 셀에 입력이 들어가고 이는 최근 입력값에 대해서보다 이전 입력값에 대한 Gradient 절대값이 매우 작게 되는 현상들을 발생시킬 것이다. 특히 RNN 셀의 내부에서 활성 함수는 Sigmoid와 그 특징이 유사한 Tanh 함수를 사용하기 때문에 이 현상은 더욱 심할 것이다. 이 문제를 `Long Term Dependency`라고도 한다.

즉 입력 시퀀스의 길이가 길어질수록 긴 입력 시퀀스에 대해서 고정된 크기의 은닉 상태 벡터로 정보를 입력함에 있어서 정보의 손실이 발생하고 이는 너무 긴 길이의 의존성, 다시 말해 Long Term Dependency를 적절히 인코딩하기 어려워  현재 입력값에 비해서 이전 입력값에 대한 훈련 정보는 점차 사라진다는 결과로 나타난다. 결국 이로 인하여 모델이 시퀀스를 바라볼때 최근 입력값에만 집중하는 현상이 벌어진다. 이는 기계 번역 뿐 아니라 문맥을 활용해야하는 다양한 자연어 처리 태스크에서 그 문제를 발생시키곤 했다.

## Attention 메커니즘
이러한 seq2seq 모델의 문제점을 해결하기 위해서 등장한 것이 바로 `Attention 메커니즘`이다. 기존 seq2seq 모델의 단점이었던 너무 멀리 떨어진 입력값에 대한 영향력이 너무 낮아진다는 문제점을 해결하기 위해서 멀리 떨어진 입력값에도 충분한 가중치를 부여하여 디코딩 과정에서 활용하겠다는 의도가 반영된 것이 바로 이 Attention 메커니즘이다. Attention 메커니즘은 본 블로그의 포스트들([[Attention] Bahdanau Attention 개념 정리](https://hcnoh.github.io/2018-12-11-bahdanau-attention), [[Attention] Luong Attention 개념 정리](https://hcnoh.github.io/2019-01-01-luong-attention))을 참고하면 더 자세히 확인할 수 있다.

![](assets/img/2023-02-17-nlp-01/2023-02-20-15-38-47.png)

심층 신경망 모델은 기본적으로 주어진 특징을 적절한 은닉 벡터로 인코딩하는 과정을 수행하려고 한다. 이 과정에서 더 중요한 특징들을 스스로 찾아서 집중하는 현상이 일어나는데 Attention 메커니즘은 이를 좀 더 수월하게끔 구조적으로 설계하는 방법론이다. Long Term Dependecy를 반영하기에는 Vanishing Gradient 문제가 발생하기에 모델이 스스로 가중치를 선택하여 Vanishing Gradient 문제가 발생하는 긴 경로를 생략하고 직접적으로 멀리 떨어진 입력값에 집중할 수 있도록 도와주는 것이 Attention 메커니즘이다.

Attention 메커니즘은 기본적으로 `Attention 점수`를 계산하는 것으로부터 출발한다. 이는 일반적으로 현재 주어진 디코더의 입력값에 대한 은닉 벡터와 입력값 전체에 대한 은닉 벡터들 사이의 내적을 통해서 계산한다. 위의 그림에서 인코더 입력 토큰들 "I", "Love", "You", "\<EOS\>" 토큰들에 대한 인코더 은닉 벡터를 각각 $$\mathbf{h}_1, \mathbf{h}_2, \mathbf{h}_3, \mathbf{h}_4$$라고 하자. 또한 디코더 입력 토큰들인 "\<BOS\>", "나는", "너를", "사랑해" 각각에 대한 디코더 은닉 벡터를 각각 $$\mathbf{s}_1, \mathbf{s}_2, \mathbf{s}_3, \mathbf{s}_4$$라고 하자. 여기서 디코더 입력 토큰 "나는"에 대한 인코더 입력 토큰 "You"의 Attention 점수는 다음과 같다:

$$
a_{2}^{(3)} = \frac{\exp\left( \mathbf{s}_2^\text{T} \mathbf{h}_3 \right)}{\sum_{j=1}^4 \exp \left( \mathbf{s}_2^\text{T}\mathbf{h}_j \right) }.
$$

여기서 디코더 입력 토큰 "나는"에 대한 모든 Attention 점수의 벡터화인 `Alignment 벡터`는 다음과 같이 정의한다:

$$
\mathbf{a}_2 = \left[ a_2^{(1)}, a_2^{(2)}, a_2^{(3)}, a_2^{(4)} \right] = \text{Softmax} \left( \left( \mathbf{s}_2^\text{T} \mathbf{h}_j \right)_{j=1}^4 \right) \in \mathbb{R}^4.
$$

여기서는 입력 토큰 "나는"에 대한 Attention 점수를 뽑은 것이기에 아마도 인코더 입력 토큰 "I"의 Attention 점수가 가장 높을 것이라고 추측할 수 있다. 좌우지간 Alignment 벡터는 인코더의 입력 토큰 시퀀스의 길이가 $$L$$인 경우에 디코더의 $$t$$번째 입력 토큰에 대해서 다음과 같이 일반적으로 정의할 수 있다:

$$
\mathbf{a}_t = \left[ a_t^{(1)}, \cdots a_t^{(L)} \right] = \text{Softmax} \left( \left( \mathbf{s}_t^\text{T} \mathbf{h}_j \right)_{j=1}^L \right) \in \mathbb{R}^L.
$$

이렇게 정의된 Alignment 벡터 $$\mathbf{a}_t$$는 디코더 입력의 $$t$$단계에서의 토큰에 대한 인코더 입력 토큰들 전체에 대해 Attention 점수를 나타내는 벡터라고 보면 된다. 모두들 알고 있듯이 $$\mathbf{a}_t$$는 정상화(Normalization)되어있기 때문에 모든 성분의 합이 1인 것을 확인할 수 있다. 이렇게 구해전 Attention 점수들은 인코더 은닉 벡터들인 $$\left\{ \mathbf{h}_j \right\}_{j=1}^L$$와 가중 합계로 계산되어서 `문맥 벡터`(Context 벡터)라고 불리우는 표현으로 변환된다:

$$
\mathbf{c}_t = \sum_{j=1}^L \mathbf{s}_t^\text{T}\mathbf{h}_j.
$$

이 문맥 벡터는 디코더가 현재 단계 $$t$$에서의 디코더 출력을 생성하는 떄에 사용되게 된다. 즉 이를 통해서 모델은 문맥 벡터로 활용될 인코더 입력 토큰들에 대한 정보를 어느 정도 수준으로 선택할지 가중치를 스스로 부여할 수 있도록 학습이 이루어지게 되며 결과적으로 매우 긴 입력 시퀀스에 대해서도 이전보다는 높은 수준으로 Long Term Dependency를 잘 반영하여 정보를 인코딩할 수 있다.

## Transformer

### Query, Key, Value

### Self-Attention

### Multihead Attention

## 참고 자료
- 

## 수정 사항
- 2023.02.25
    - 최초 게제
- 2023.02.27
    - Attention 메커니즘 내용 정리