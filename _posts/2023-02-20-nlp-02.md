---
layout: post
use_math: true
title: "[Natural Language Processing] 2. Word2Vec"
date: 2023-02-20 00:00:00
tagline: "자연어 처리에서의 기초 작업 단어 임베딩을 위한 Word2Vec 방법론에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-02-20-nlp-02
---

이번 포스트에서는 인공 신경망 기반 자연어 처리 방법론에서 기본적으로 활용되는 단어 임베딩 방법론인 `Word2Vec`에 대해서 정리해보도록 한다.

## 단어 임베딩
이전 포스트에서 간단히 설명하였듯 인공 신경망 기반의 자연어 처리 방법론들은 기본적으로 토큰(단어)에 대한 벡터화된 표현(Representation) 방식을 따른다. 이를 `단어 임베딩`(Word Embedding)이라고 부르며 우리가 원하는 특징을 지닌 벡터 공간으로 토큰을 투영하여 표현하게 된다. 단어 임베딩이라고 부르는 이유는 영어 문장의 토큰화는 단어(Word) 단위로 수행이 되며 이 단어 토큰에 대한 표현을 구해낸다는 의미로 단어 임베딩이라고 부르게 된다.

우리가 원하는 벡터 공간의 특징은 기본적으로 `Distributional Hypothesis`라는 대전제를 따르게 된다. 이는 말뭉치 내에서 가깝게 등장하는 단어들은 그 의미상으로도 유사할 것이라는 가정이다. 실제로 `Word2Vec`에서는 이 가정을 통해서 훈련을 수행하였고 결과물도 꽤 의미가 있어 보인다.

## Skip-Gram 모델
`Skip-Gram` 모델은 기본적으로 좋은 단어 표현을 위한 모델이며 주어진 단어를 바탕으로 주위에 있는 단어들이 무엇일지를 예측할 수 있는 모델이다. 이는 지극히 자연스럽게 Distributional Hypothesis를 따른다.

좀 더 형식적으로는, 주어진 단어들의 나열 $$w_1, w_2, \cdots, w_T$$에 대하여 Skip-Gram 모델의 목적 함수(Objective Function)은 다음과 같은 로그 확률의 평균으로 정의한다:

$$
\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} \vert w_t).
$$

여기서 $$c$$는 훈련용 문맥(Training Context)의 크기이며 이 훈련용 문맥은 중간 단어 $$w_t$$에 대한 함수이다. 더 큰 크기의 훈련용 문맥을 통해 훈련한다면 더 높은 정확도를 기대할 수 있겠지만 그 만큼 훈련 시간이 오래 걸릴 것이다. Skip-Gram 모델을 통해서 다루는 $$p(w_{t+j} \vert w_t)$$는 기본적으로 다음의 형태를 지닌다:

$$
p(w_O \vert w_I) = \frac{\exp \left( \mathbf{u}_{w_O}^\text{T} \mathbf{v}_{w_I} \right)}{\sum_{w=1}^W \exp \left( \mathbf{u}_w^\text{T} \mathbf{v}_{w_I} \right)}.
$$

이는 `Softmax 함수`를 활용하는 간단한 접근이다. 또한 여기서 $$\mathbf{v}_w$$와 $$\mathbf{u}_w$$은 각각 단어 $$w$$에 대한 입력/출력 벡터 표현이고, $$W$$는 단어 사전의 총 단어 개수이다.

Softmax 함수를 활용한 이러한 방식의 확률 계산법은 사실 굉장히 큰 문제가 있다. 바로 이 계산의 계산 복잡도가 단어 사전의 크기인 $$W$$만큼 크기 때문이다. 그 이유는 Softmax 함수의 분모 때문이다. 전체 단어 사전을 순회하면서 모든 값들을 더해줘야 하는 계산 방식이 바로 그 문제의 핵심이다. 일반적을 단어 사전의 크기는 작게는 $$10^5$$에서 크게는 $$10^7$$에 이를 정도로 매우 크다. 이는 엄청난 부하를 일으키게 될 것이다.

이러한 계산 복잡도 문제를 해결하기 위한 방향으로 일반적으로는 크게 2가지 방법론이 존재한다. 첫 번째는 `Hierarchical Softmax`라는 방법이고, 두 번째는 `Negative Sampling`이라는 방법이다.

## Hierarchical Softmax
`Hierarchical Softmax`는 계산 복잡도 $$W$$인 Softmax의 연산을 $$\log_2 W$$로 줄일 수 있는 방법이다. 즉 $$W$$개의 전체 단어를 탐색하는 것이 아닌 $$\log_2 W$$개의 단어만을 탐색하는 방식이라는 것이다. 이를 위해서는 기본적으로 이진 트리 구조의 표현을 만들어야 한다.

## 참고 자료
- [Distributed Representations of Words and Phrases
and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Hierarchical Probabilistic Neural Network Language Model](http://proceedings.mlr.press/r5/morin05a/morin05a.pdf)

## 수정 사항
- 2023.02.20
    - 최초 게제