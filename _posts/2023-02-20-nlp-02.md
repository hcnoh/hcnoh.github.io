---
layout: post
use_math: true
title: "[Natural Language Processing] 2. Word2Vec"
date: 2023-02-20 00:00:00
tagline: "자연어 처리에서의 기초 작업 단어 임베딩을 위한 Word2Vec 방법론에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- probability
- statistics
- machine learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-02-20-nlp-02
---

이번 포스트에서는 인공 신경망 기반 자연어 처리 방법론에서 기본적으로 활용되는 단어 임베딩 방법론인 `Word2Vec`에 대해서 정리해보도록 한다.

## 단어 임베딩
이전 포스트에서 간단히 설명하였듯 인공 신경망 기반의 자연어 처리 방법론들은 기본적으로 토큰(단어)에 대한 벡터화된 표현(Representation) 방식을 따른다. 이를 `단어 임베딩`(Word Embedding)이라고 부르며 우리가 원하는 특징을 지닌 벡터 공간으로 토큰을 투영하여 표현하게 된다. 단어 임베딩이라고 부르는 이유는 영어 문장의 토큰화는 단어(Word) 단위로 수행이 되며 이 단어 토큰에 대한 표현을 구해낸다는 의미로 단어 임베딩이라고 부르게 된다.

우리가 원하는 벡터 공간의 특징은 기본적으로 `Distributional Hypothesis`라는 대전제를 따르게 된다. 이는 말뭉치 내에서 가깝게 등장하는 단어들은 그 의미상으로도 유사할 것이라는 가정이다. 실제로 `Word2Vec`에서는 이 가정을 통해서 훈련을 수행하였고 결과물도 꽤 의미가 있어 보인다.

## Skip-Gram 모델
`Skip-Gram` 모델은 기본적으로 좋은 단어 표현을 위한 모델이며 주어진 단어를 바탕으로 주위에 있는 단어들이 무엇일지를 예측할 수 있는 모델이다. 이는 지극히 자연스럽게 Distributional Hypothesis를 따른다.

좀 더 형식적으로는, 주어진 단어들의 나열 $$w_1, w_2, \cdots, w_T$$에 대하여 Skip-Gram 모델의 목적 함수(Objective Function)은 다음과 같은 로그 확률의 평균으로 정의한다:

$$
\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} \vert w_t).
$$

여기서 $$c$$는 훈련용 문맥(Training Context)의 크기이며 이 훈련용 문맥은 중간 단어 $$w_t$$에 대한 함수이다. 더 큰 크기의 훈련용 문맥을 통해 훈련한다면 더 높은 정확도를 기대할 수 있겠지만 그 만큼 훈련 시간이 오래 걸릴 것이다. Skip-Gram 모델을 통해서 다루는 $$p(w_{t+j} \vert w_t)$$는 기본적으로 다음의 형태를 지닌다:

$$
p(w_O \vert w_I) = \frac{\exp \left( \mathbf{u}_{w_O}^\text{T} \mathbf{v}_{w_I} \right)}{\sum_{w=1}^W \exp \left( \mathbf{u}_w^\text{T} \mathbf{v}_{w_I} \right)}.
$$

이는 `Softmax 함수`를 활용하는 간단한 접근이다. 또한 여기서 $$\mathbf{v}_w$$와 $$\mathbf{u}_w$$은 각각 단어 $$w$$에 대한 입력/출력 벡터 표현이고, $$W$$는 단어 사전의 총 단어 개수이다. 여기서 입력/출력 벡터 표현이 서로 다른 이유는 모든 단어들은 자신이 입력 단어 위치에 있는 경우와 출력 단어 위치에 있는 경우에 그 역할이 달라야 한다는 가정이 담겨 있다. 벡터 표현이 전혀 다름으로써 그 역할의 다름을 반영할 수 있는 것이다.

Softmax 함수를 활용한 이러한 방식의 확률 계산법은 사실 굉장히 큰 문제가 있다. 바로 이 계산의 계산 복잡도가 단어 사전의 크기인 $$W$$만큼 크기 때문이다. 그 이유는 Softmax 함수의 분모 때문이다. 전체 단어 사전을 순회하면서 모든 값들을 더해줘야 하는 계산 방식이 바로 그 문제의 핵심이다. 일반적을 단어 사전의 크기는 작게는 $$10^5$$에서 크게는 $$10^7$$에 이를 정도로 매우 크다. 이는 엄청난 부하를 일으키게 될 것이다.

이러한 계산 복잡도 문제를 해결하기 위한 방향으로 일반적으로는 크게 2가지 방법론이 존재한다. 첫 번째는 `Hierarchical Softmax`라는 방법이고, 두 번째는 `Negative Sampling`이라는 방법이다.

## Hierarchical Softmax
`Hierarchical Softmax`는 계산 복잡도 $$W$$인 Softmax의 연산을 $$\log_2 W$$로 줄일 수 있는 방법이다. 즉 $$W$$개의 전체 단어를 탐색하는 것이 아닌 $$\log_2 W$$개의 단어만을 탐색하는 방식이라는 것이다. 이를 위해서는 기본적으로 단어 사전에 속한 단어들을 노드로 하는 이진 트리 구조의 표현을 구축해야 한다.

### 이진 트리
`이진 트리`(Binary Tree)는 트리(Tree) 형태의 자료 구조의 한 종류이다. 기본적으로 트리는 노드(Node)와 방향이 있는 에지(Edge)로 이루어진 유향 그래프 (Directed Graph) 형태의 자료 구조의 한 종류이며 하나의 노드에서 출발하여 다른 노드들을 순회하여 자기 자신에게 되돌아오는 순환 연결이 없는 그래프이다. 이러한 형태의 그래프를 유향 비순환 그래프(Directed Acyclic Graph, DAG)라고도 한다. 트리는 이러한 DAG 중 모든 노드에 대한 부모 노드가 항상 한 개 이하인 경우를 말한다. 트리는 계층화된 노드들을 표현하는데 적절한 형태의 자료 구조이다.

이진 트리는 이러한 트리들 중 모든 노드들에 대하여 항상 자식이 2개 이하인 트리를 이진 트리라고 한다. 이진 트리는 인덱싱이 되어있는 자료들에 대해서 탐색 알고리즘에 아주 효율적인 구조인데 모든 노드들을 탐색하며 원하는 노드를 찾는 방식이 아닌 트리의 깊이만큼만 검색을 하면 되기 때문이다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-11-19-30.png)

위 그림은 배열 [1, 2, 4, 5, 6, 7, 10]에서 6을 탐색하는 과정을 일반 배열에서와 배열을 적절한 이진 트리로 구성하여 진행하는 과정을 나타내고 있다. 일반 배열에서는 결국 요소 하나하나를 살펴보면서 내가 찾고자 하는 값인 6과 일치하는지 여부를 확인하여 탐색을 진행하게 된다. 이는 최악의 경우에 모든 요소를 다 살펴보는 결과로 이끌게 되며 이는 계산 복잡도가 $$O(n)$$으로 표현될 수 있다. 여기서 $$n$$은 찾고자 하는 대상이 되는 자료 구조의 크기이며 위의 예시의 경우 배열의 크기인 7이 될 것이다.

반면 이진 트리의 경우를 살펴보자. 먼저 루트(Root) 노드인 5와 찾고자 하는 노드인 6의 크기를 비교한다. 여기서 6이 5보다 크기 때문에 오른쪽에 있는 자식 노드의 방향으로 탐색을 재개한다. 그 다음으로 다시 현재의 노드인 7과 비교하며 6은 7보다 더 작기 때문에 왼쪽 자식 노드인 6을 찾아낼 수 있다. 이 경우 일반 배열에서처럼 6번 탐색을 한 것이 아니라 총 3번 탐색을 하였으며, 최악의 경우에도 트리의 최대 깊이를 넘지 않고 탐색이 가능하다.

그러면 이진 트리의 최대 깊이는 어떻게 구할 수 있을까? 계산 복잡도를 논하기 위해서 이진 트리의 최대 깊이가 가장 큰 상황을 고려해야 할 것이다. 이를 모든 부모 노드들에 대해서 자식 노드의 개수가 2인 정 이진 트리(Full Binary Tree)의 형태로부터 확인할 수 있다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-12-41-15.png){: width="50%" height="50%"}

위의 그림은 최대 깊이가 $$k$$인 정 이진 트리의 전체 노드 개수를 구하는 과정을 설명하기 위한 그림이다. 단계를 하나씩 내려가며 확인할 수 있는 단계별 노드의 개수는 현재 단계 $$l$$에 대해서 $$2^{l - 1}$$개인 것을 확인할 수 있다. 즉 최대 깊이가 $$k$$인 정 이진 트리의 전체 노드 개수는 다음과 같다:

$$
\sum_{l=1}^k 2^{l - 1} = 2^k - 1.
$$

만약 전체 노드의 개수가 $$n$$이라면 이 이진 트리가 가질 수 있는 최대 깊이는 역으로 다음과 같이 구할 수 있다:

$$
n = 2^k - 1. \ \ \ \ \Longrightarrow \ \ \ \ k = \log_2(n + 1) = O\left( \log_2(n) \right).
$$

즉, 최악의 경우에도 이진 트리를 활용하면 계산 복잡도는 $$O\left( \log_2(n) \right)$$이 된다. 이는 계산 복잡도가 $$O(n)$$인 일반 배열보다 훨씬 계산 복잡도가 낮다.

### 허프만 코딩
`허프만 코딩`(Huffman Coding)은 원래 정보 이론 분야에서 데이터를 구성하는 각각의 심벌의 등장 빈도를 통해서 이 데이터를 압축하는 방법론 중 하나이다. 허프만 코딩에서는 먼저 빈도수를 활용하여 각 심벌들을 리프(Leaf) 노드로 하는 이진 트리를 구축하는데 Hierarchical Softmax에서는 이 방법을 이용하여 단어 사전의 이진 트리 구축을 수행한다.

그러면 단어 사전의 이진 트리를 어떻게 구축하는지 알아보자. 기본적으로 허프만 코딩을 통해서 생성되는 이진 트리는 빈도수를 기준으로 빈도수가 낮은 노드의 깊이를 더 깊게 가져가려고 한다. 그 이유는 빈도수가 높은 노드의 탐색의 깊이를 낮게하고 빈도수가 낮은 노드의 탐색의 깊이를 깊게 함으로써 평균적인 탐색 횟수를 줄이고자 하기 때문이다. 이를 위해서는 먼저 주어진 말뭉치로부터 단어 사전과 그 단어 각각의 빈도수를 추출한 이후에 구축을 시작해야 한다. 여기서는 예시를 위해서 단어 사전이 A, B, C, D, E, F라는 단어로 구성되어 있고 이들 빈도수가 각각 2, 3, 5, 7, 9, 13이라고 가정하자. 가장 먼저 할 작업은 빈도수를 기준으로 이 단어들을 정렬하는 것이다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-19-52.png){: width="50%" height="50%"}

정렬된 단어들 중에서 가장 빈도수가 낮은 단어부터 두 단어씩 묶어서 트리를 형성하기 시작한다. 현재 예시에서는 단어 A, B를 먼저 묶어야 할 것이다. 두 단어를 묶고 나서 두 단어의 부모 노드를 만들고 이 부모 노드의 빈도수는 단어 A, B의 합인 5로 설정한다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-21-22.png){: width="50%" height="50%"}

이제 여기부터 그 다음 빈도수의 단어와 비교를 한다. 현재 노드의 빈도수 5와 단어 C의 빈도수를 비교하여 단어 C의 빈도수가 5보다 더 높거나 같으면 현재 노드와 단어 C를 또 묶어서 새로운 부모 노드 휘하로 구성한다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-23-22.png){: width="50%" height="50%"}

이제 빈도수 총합 10인 부모 노드와 그 다음 단어 D의 빈도수를 또 비교한다. 만약 단어 D의 빈도수가 10보다 작다면 그 다음 단어인 E의 빈도수와 더하여 그 값을 10과 비교해본다. 여기서는 16이 될 것이고 이는 10보다는 높기 때문에 16 빈도수의 새로운 부모 노드를 형성하는 이진 서브 트리를 구성하여 오른쪽으로 정렬한다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-27-18.png){: width="50%" height="50%"}

이제 마지막으로 남은 단어 F는 빈도수를 두 부모 노드의 빈도수인 10, 16과 비교하여 적절한 위치에 두고 이진 서브 트리를 구성한다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-29-48.png){: width="50%" height="50%"}

최종적으로 형성된 이진 트리는 다음과 같다.

![](assets/img/2023-02-20-nlp-02/2023-02-21-22-30-12.png){: width="50%" height="50%"}

### Hierarchical Softmax
이제 본격적으로 Hierarchical Softmax에 대해서 다뤄보자. Hierarchical Softmax의 기본적인 원리는 단어 각각에 대해서 자신의 임베딩 벡터와 비교하고자 하는 다른 단어의 임베딩 벡터 사이의 유사도를 측정하는 대신, 이진 트리 상에서 비교하고자 하는 다른 단어로의 경로상의 노드들의 임베딩 벡터와의 유사도를 측정하는 방식으로 유사도 계산량을 줄이겠다는 것이다.

설명만 들으면 복잡하기 때문에 예시를 활용해보도록 하자. 위에서 허프만 코딩을 설명하기 위해서 사용했던 단어 A, B, C, D, E, F에 대한 이진 트리를 바탕으로 설명을 진행해보자. 우리는 이를 위해서 $$p(\text{C} \vert \text{E})$$를 구하는 과정을 따라가 볼 것이다. 즉, 입력 단어는 E이고 출력 단어는 C인 경우이다.

![](assets/img/2023-02-20-nlp-02/2023-02-23-21-59-17.png){: width="50%" height="50%"}

일단 가장 상단에 있는 루트 노드와 임베딩 벡터 사이의 유사도를 먼저 측정한다. 루트 노드의 임베딩 벡터를 $$\mathbf{u}_{39}$$라고 한다면 유사도 계산은 다음과 같다:

$$
\sigma \left( \mathbf{u}_{39}^\text{T} \mathbf{v}_\text{E} \right).
$$

여기서 $$\sigma$$는 `로지스틱 함수`(Logistic Function) 또는 `시그모이드 함수`(Sigmoid Function)라고 불리우는 함수이다. 그 다음으로 단어 C로의 경로를 따라가며 만나는 다음 노드인 23 노드와 유사도를 계산한다:

$$
\sigma \left( \mathbf{u}_{23}^\text{T} \mathbf{v}_\text{E} \right).
$$

그리고 단어 C로의 경로상에서 만날 수 있는 그 다음 노드인 10 노드와 유사도를 계산한다:

$$
\sigma \left( \mathbf{u}_{\text{C}}^\text{T} \mathbf{v}_\text{E} \right).
$$

이렇게 구한 유사도들을 바탕으로 최종적으로 우리가 구하고자 했던 $$p(\text{C} \vert \text{E})$$는 다음과 같이 계산한다:

$$
p(\text{C} \vert \text{E}) = \sigma \left( \mathbf{u}_{39}^\text{T} \mathbf{v}_\text{E} \right) \cdot \sigma \left( \mathbf{u}_{23}^\text{T} \mathbf{v}_\text{E} \right) \cdot \sigma \left( \mathbf{u}_{10}^\text{T} \mathbf{v}_\text{E} \right).
$$

이 과정을 일반적으로 기술하면 다음과 같이 쓸 수 있다:

$$
p(w \vert w_I) = \prod_{j = 1}^{L(w) - 1} \sigma \left( \left[ \left[ n(w, j + 1) = \text{ch}(n(w, j)) \right] \right] \cdot \mathbf{u}_{n(w, j)}^\text{T} \mathbf{v}_{w_I} \right).
$$

여기서 $$n(w, j)$$는 루트부터 단어 $$w$$까지의 경로상에서 $$j$$번째 노드를 의미하고, $$L(w)$$는 이 경로의 길이이다. 즉 $$n(w, 1)$$은 루트 노드이고 $$n(w, L(w))$$는 $$w$$가 될 것이다. 또한 모든 경로상의 노드 $$n$$에 대하여 $$\text{ch}(n)$$은 노드 $$n$$의 어떤 임의의 자식 노드를 의미하고, $$[[x]]$$는 $$x$$가 참이면 1, 거짓이면 -1을 반환하는 지시 함수이다(Indicator Function).

개인적으로 이 수식에서 이미 루트로부터 단어 $$w$$까지의 경로를 고정하였기 때문에 저 지시 함수가 왜 필요한지는 잘 모르겠지만 어쨌든 논문에 나와있는 식은 위와 같다. 이런 방식으로 계산을 한다면 $$p(w_O \vert w_I)$$를 계산하기 위해서 필요한 비용은 $$L(w_O)$$일 것이고 이는 절대 $$\log_2(W)$$를 넘지 않게 된다.

## 참고 자료
- [Distributed Representations of Words and Phrases
and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Hierarchical Probabilistic Neural Network Language Model](http://proceedings.mlr.press/r5/morin05a/morin05a.pdf)
- [트리와 이진 트리](https://ratsgo.github.io/data%20structure&algorithm/2017/10/21/tree/)
- [Huffman Coding](https://en.wikipedia.org/wiki/Huffman_coding)
- [(Java) 허프만 코드(Huffman Coding) 개념 및 구현](https://wkdtjsgur100.github.io/huffman/)

## 수정 사항
- 2023.02.20
    - 최초 게제
- 2023.02.23
    - 이진 트리 및 허프만 코딩 내용 추가
- 2023.02.24
    - Hierarchical Softmax 내용 추가