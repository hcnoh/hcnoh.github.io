---
layout: post
use_math: true
title: "[자연어 처리] 4. BERT"
date: 2023-03-07 00:00:00
tagline: "자연어 처리에서 최근 가장 많이 활용되는 거대 언어 모델의 시초인 BERT에 대해서 정리"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-03-07-nlp-04
---

이번 포스트에서는 최근 자연어 처리 분야에서 가장 많이 활용되고 있는 거대 언어 모델의 아이디어를 처음으로 제안했단 `BERT`에 대해서 다룬다. BERT는 등장하자마자 획기적이라는 평가를 받았던 언어 모델이며 기존에 존재했던 대부분의 자연어 처리 태스크에서 `SOTA`(State-of-the-Art) 수준을 달성하는 강력한 모습을 보였었다. BERT의 구조와 훈련 방식을 따라가는 구성으로 이번 포스트의 내용을 다뤄볼 계획이다.

## 목차
- [기존 언어 모델 소개](#기존-언어-모델-소개)
    - [ELMo](#elmo)
    - [OpenAI GPT](#openai-gpt)

## 기존 언어 모델 소개
거대 언어 모델 연구는 지금까지 언어 모델에 대한 `사전 학습`(Pre-Training)을 바탕으로 토큰 표현(Representation)을 잘 획득하고 이를 통해 다양한 자연어 처리 태스크를 수행하는 것을 목적으로 수행되어 왔다. 이러한 방향의 언어 모델 연구 역사는 꽤나 오래되었지만 크게 3가지 공통된 방향성이 존재한다.

### ELMo
우선 비지도 특징 기반 접근이 있다. 이 접근은 비지도 기반 특징화를 통해 단어 또는 토큰의 좋은 표현을 획득하려고 했던 연구들을 말한다. 일반적으로는 `정방향`(Left-to-Right) 언어 모델링 기반 목적 함수 최적화를 통해 단어/토큰 표현을 획득했었고 이런 방식은 정방향 언어 모델링은 문장 임베딩(Sentence Embedding), 더 나아가 문단 임베딩(Paragraph Embedding)으로 일반화되어 왔다.

`ELMo`(Embeddings from Language Models)는 이러한 비지도 특징 기반 접근의 대표적인 언어 모델이다. 기존의 연구들과는 다르게 ELMo는 문장 내에서 최대한 문맥 정보를 추출할 수 있는 방법을 통해 단어 표현을 고도화하고자 하였다. 우선 정방향 언어 모델링 뿐 아니라 `역방향`(Right-to-Left) 언어 모델링을 통한 정보를 둘다 활용하여 `양방향`(Bidirectorional) 언어 모델링을 구현하고자 하였다.

![ELMo](assets/img/2023-03-07-nlp-04/elmo.png)

목적 함수 계산을 위한 ELMo의 양방향 언어 모델링은 다음과 같이 수행된다. 우선 아래와 같이 N개의 토큰이 주어졌다고 가정해 보자.

$$
(t_1, t_2, \cdots, t_N)
$$

여기서 정방향 언어 모델링을 통한 토큰 시퀀스 확률 값은 다음과 같다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N p(t_k | t_1, t_2, \cdots, t_{k-1})
$$

마찬가지로 역방향 언어 모델링을 통한 토큰 시퀀스 확률 값도 계산할 수 있다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=N}^1 p(t_k | t_{k+1}, t_{k+2}, \cdots, t_{N})
$$

이러한 언어 모델링을 통해서 목적 함수는 Maximum Likelihood Estimation에 의하여 다음과 같이 정리될 수 있다:

$$
\sum_{k=1}^N \left( \log p(t_k | t_1, \cdots, t_{k-1} ; \boldsymbol{\theta}) + \log p(t_k | t_{k + 1}, \cdots, t_N ; \boldsymbol{\theta}) \right)
$$

또한 ELMo는 LSTM 계층을 여럿 쌓은 스택에서 마지막 계층의 은닉 상태(Hidden State) 벡터만을 특징으로 사용하였던 기존의 접근과는 다르게 스택의 모든 계층의 으닉 상태 벡터를 활용하여 특징화를 수행한다. 이는 더 깊게 문맥 정보를 활용할 수 있다는 장점이 있다고 한다.

ELMo의 LSTM 계층이 L층으로 구성되어 있는 경우 토큰 $$t_k$$에 대한 표현 집합은 다음과 같다:

$$
R_k = \left\{ \mathbf{x}_k, \overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j} | j=1,2, \cdots, L \right\}
$$

여기서 $$\mathbf{x}_k$$는 토큰 $$t_k$$의 문맥에 독립적인 표현이다. 그리고 $$\overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j}$$는 각각 $$j$$번째 정방향/역방향 은닉 상태 벡터를 의미한다. 표현 집합은 이어 붙어서(Concatenation) 하나의 벡터로 형성되어 표현 집합을 새롭게 정의하게 된다.

$$
R_k = \left\{ \mathbf{h}_{k, j} | j=1,2, \cdots, L \right\}
$$

ELMo의 최종 표현은 이 표현 집합에 포함된 L개의 표현을 가중합을 통해 계산되어 Task-Specific한 계층의 입력에 활용되어 우리가 원하는 태스크에 활용된다.

이러한 과정은 ELMo의 모델 구조에서도 확인할 수 있다. LSTM 스택 2개가 각각 정방향과 역방향으로 주어진 토큰의 임베딩 시퀀스 $$(E_1, E_2, \cdots, E_N)$$을 은닉 벡터 시퀀스로 인코딩을 수행하고 이 결과를 최종 은닉 벡터 시퀀스 $$(T_1, T_2, \cdots, T_N)$$으로 결합하여 구성한다.

### OpenAI GPT
그 다음으로는 비지도 파인 튜닝(Fine-Tunning) 접근이 있다. 파인 튜닝이란 사전 학습된 모델의 가중치에 Task-Specific한 새로운 데이터를 사전 학습용 데이터에 비해서 상대적으로 소량 활용하여 Task-Specific한 모델 가중치를 획득하는 작업을 말한다.

초기 연구들은 비지도 파인 튜닝 접근을 위하여 사전 학습된 단어(토큰) 임베딩을 활용하는 정도였다. 반면 최근 연구들은 비지도 방식을 통해서 문장, 또는 문서에 대한 임베딩 모델에 대해 사전 학습을 시도하고 있다. 이러한 접근의 좋은 점은 매우 적은 파라미터만이 업데이트되는 정도라서 파인 튜닝 과정이 매우 짧다는 점이다.

`OpenAI GPT`는 이러한 문장 수준의 사전 학습을 통해서 GLUE 벤치마크에서 다양한 문장 수준의 태스크에 대해 SOTA를 달성하였다.

![OpenAI GPT](assets/img/2023-03-07-nlp-04/gpt.png)

`GPT`(Generative Pretrained Transformers)는 이름에서 알 수 있듯 Transformer 구조로 되어 있다. GPT의 컨셉은 기본적으로 준지도 학습(Semi-Supervised Learning)이다. 이는 비지도 사전 학습과 지도 학습 기반의 지도 파인 튜닝을 활용하기 때문이다.

GPT의 `비지도 사전 학습`은 다음과 같은 목적 함수를 최적화하는 방식으로 진행된다. 우선 주어진 레이블이 없는 토큰들의 말뭉치 $$\mathcal{U} = (u_1, u_2, \cdots, u_n)$$에 대하여 다음의 Likelihood Function을 목적 함수로 한다:

$$
\sum_i \log p(u_i | u_{i - k}, \cdots, u_{i - 1}; \boldsymbol{\theta})
$$

여기서 k는 문맥 윈도우 크기이다. 목적 함수의 조건부 확률은 언어 모델을 통해서 계산된다. GPT 논문의 저자들은 이 조건부 확률 계산을 위하여 다층 Transformer 디코더를 사용하였다고 한다.

그 다음으로 GPT의 `지도 파인 튜닝`은 다음과 같이 진행된다. 우선 주어진 레이링된 데이터싯 $$\mathcal{C}$$를 가정하자. 이 데이터셋의 각 인스턴스는 입력 토큰 시퀀스 $$(x_1, x_2, \cdots, x_m)$$과 레이블 $$y$$로 구성된다.

Transformer 디코더의 최종 출력 $$\mathbf{h}$$은 이후 Task-Specific 추가 선형 계층에 입력되어 최종적으로 레이블 $$y$$를 다음과 같이 예측한다:

$$
p(y|x_1, \cdots, x_m) = \text{softmax}(\mathbf{hW}_y)
$$

이는 Maximum Likelihood Estimation을 통해 다음과 같은 목적 함수 계산에 활용된다:

$$
\sum_{\mathbf{x}, y} \log p(y | x_1, \cdots, x_m)
$$

### 지도 데이터를 통한 전이 학습
지도 데이터(Supervised Data)를 통한 `전이 학습`(Transfer Learning) 과정은 위에서 설명하였던 ELMo 및 GPT에서도 Task-Specific한 계층의 학습을 위하여 활용되어 왔다. 이런 방식을 통해 거대 말뭉치로부터 사전 학습된 모델을 활용하여 적은 학습 비용을 바탕으로 파인 튜닝을 수행하여 자연어 추론이나 기계 번역 등의 새로운 태스크에 대해 좋은 성능을 낼 수 있는 모델을 전이 학습을 시킬 수 있다.

이러한 전이 컴퓨터 비전 분야에서 증명되어 왔다. 이미지넷(ImageNet) 등과 같은 거대 데이터를 통해 사전 학습된 모델에 파인 튜닝을 통한 전이 학습은 그 학습 방법이 유효하다는 것을 성능으로 확인할 수 있다.

## BERT 소개
`BERT`(Bidirection Encoder Representations from Transformers)는 구글에서 2018년에 발표한 거대 언어 모델이다. 이름에서 알 수 있듯 BERT는 `Transformer` 기반의 인코더 구조를 지닌 언어 모델이며 따라서 토큰, 또는 문장, 더 나아가 문서의 은닉 표현(Hidden Representation)을 학습하는 모델이라고 볼 수 있다.

![BERT](assets/img/2023-03-07-nlp-04/bert.png)

위 그림은 BERT의 구조를 간단히 설명하는 그림이다. 위 그림에서 확인할 수 있듯이 기본적으로 Transformer로 구성되어 있음을 확인할 수 있으며 Transformer의 특별한 구조를 활용하여 양방향으로 정보를 활용하여 각 토큰의 인코딩을 수행하는 것을 볼 수 있다.

### BERT의 구조
BERT의 구조는 위에서 말했듯이 다층 양방향 Transformer 인코더 형태이다. BERT의 저자들은 이 구조를 구성하는 Transformer는 원래의 Transformer에서 변화없이 그대로 사용하였다고 한다. Transformer에 대해서 더 자세히 알아보고 싶으면 이전 [포스트](https://hcnoh.github.io/2023-02-25-nlp-03)를 참고하면 된다.

저자들은 BERT 모델을 크기별로 크게 2개를 제안했다. 먼저 `BERT BASE`이다. BERT BASE 말 그대로 기본 BERT이며 Transformer 스택 계층 수 $$L$$을 12로, 은닉 표현 사이즈 $$H$$를 768로, Self-Attention Head 수 $$A$$를 12로 설정한 모델이다.

그 다음으로는 `BERT LARGE`가 있다. BERT LARGE는 $$L$$을 24로, $$H$$를 1024로, $$A$$를 16으로 설정한 모델이다. 아래 표에 두 모델에 대해서 간단히 정리를 해보았다.

|            | L  | H    | A  | 파라미터 수 |
|--          |--  |--    |--  |--        |
| BERT BASE  | 12 | 768  | 12 | 110M     |
| BERT LARGE | 24 | 1024 | 16 | 340M     | 

BERT BASE에 비해서 BERT LARGE는 파라미터 수가 약 3배 정도 되는 340만 정도이다. 원래 저자들이 의도한 모델은 BERT LARGE이지만 BERT BASE는 GPT와 비교를 하기 위하여 만든 모델이라는 듯 하다.

### BERT의 입/출력 표현
BERT의 입/출력 표현 방법은 다양한 NLP 태스크를 수행하기 위한 형태로 설계되었다. 단순히 토큰 표현들 뿐 아니라 단일 문장, 그리고 질의 응답 태스크 등을 수행하기 위한 이중 문장을 입력받을 수 있는 형태이다.

우선 모든 입력 토큰은 `WordPiece`라는 이름의 임베딩을 사용했다. 모든 토큰들은 이 WordPiece 임베딩을 통해서 벡터 형태로 변환된다. 또한 추가적으로 모든 문장의 첫 번째 토큰은 [CLS] 토큰이라는 것을 붙여서 사용한다. 이 [CLS] 토큰은 문장 전체의 인코딩을 위한 토큰으로 문장 분류 등의 태스크에 활용될 수 있다.

이중 문장이 토큰 시퀀스로 입력되어 모델에 들어오는 경우 이 문장들을 구분하는 방법이 필요하다. BERT에서는 두 가지 방법을 통해서 이 구분을 수행한다. 먼저 구분자인 [SEP] 토큰을 중간에 껴넣어서 구분하는 방법이다. 두 번째는 각 토큰 임베딩에 추가로 문장 A에 속한 것인지 문장 B에 속한 것인지를 나타내는 임베딩 벡터를 이어 붙여서 모델이 구분할 수 있도록 하는 방법이다.

![BERT 입력 표현 방법](assets/img/2023-03-07-nlp-04/bert-input-representation.png)

## BERT의 사전 학습

![BERT의 사전 학습 및 파인 튜닝](assets/img/2023-03-07-nlp-04/bert-pretraining-and-finetuning.png)

### 태스크 (1) - 마스크 언어 모델

### 태스크 (2) - 다음 문장 예측

### 사전 학습 데이터

## BERT의 파인 튜닝


## 참고 자료
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
- [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised)

## 수정 사항
- 2023.03.07
    - 최초 게제