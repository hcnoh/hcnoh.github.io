---
layout: post
use_math: true
title: "[자연어 처리] 4. BERT"
date: 2023-03-07 00:00:00
tagline: "자연어 처리에서 최근 가장 많이 활용되는 거대 언어 모델의 시초인 BERT에 대해서 정리"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-03-07-nlp-04
---

이번 포스트에서는 최근 자연어 처리 분야에서 가장 많이 활용되고 있는 거대 언어 모델의 아이디어를 처음으로 제안했단 `BERT`에 대해서 다룬다. BERT는 등장하자마자 획기적이라는 평가를 받았던 언어 모델이며 기존에 존재했던 대부분의 자연어 처리 태스크에서 `SOTA`(State-of-the-Art) 수준을 달성하는 강력한 모습을 보였었다. BERT의 구조와 훈련 방식을 따라가는 구성으로 이번 포스트의 내용을 다뤄볼 계획이다.

## 목차
- [기존 언어 모델 소개](#기존-언어-모델-소개)
    - [ELMo](#elmo)
    - [OpenAI GPT](#openai-gpt)
    - [지도 데이터를 통한 전이 학습](#지도-데이터를-통한-전이-학습)

## 기존 언어 모델 소개
거대 언어 모델 연구는 지금까지 언어 모델에 대한 `사전 학습`(Pre-Training)을 바탕으로 토큰 표현(Representation)을 잘 획득하고 이를 통해 다양한 자연어 처리 태스크를 수행하는 것을 목적으로 수행되어 왔다. 이러한 방향의 언어 모델 연구 역사는 꽤나 오래되었지만 크게 3가지 공통된 방향성이 존재한다.

### ELMo
우선 비지도 특징 기반 접근이 있다. 이 접근은 비지도 기반 특징화를 통해 단어 또는 토큰의 좋은 표현을 획득하려고 했던 연구들을 말한다. 일반적으로는 `정방향`(Left-to-Right) 언어 모델링 기반 목적 함수 최적화를 통해 단어/토큰 표현을 획득했었고 이런 방식은 정방향 언어 모델링은 문장 임베딩(Sentence Embedding), 더 나아가 문단 임베딩(Paragraph Embedding)으로 일반화되어 왔다.

`ELMo`(Embeddings from Language Models)는 이러한 비지도 특징 기반 접근의 대표적인 언어 모델이다. 기존의 연구들과는 다르게 ELMo는 문장 내에서 최대한 문맥 정보를 추출할 수 있는 방법을 통해 단어 표현을 고도화하고자 하였다. 우선 정방향 언어 모델링 뿐 아니라 `역방향`(Right-to-Left) 언어 모델링을 통한 정보를 둘다 활용하여 `양방향`(Bidirectorional) 언어 모델링을 구현하고자 하였다.

![ELMo](assets/img/2023-03-07-nlp-04/elmo.png)

목적 함수 계산을 위한 ELMo의 양방향 언어 모델링은 다음과 같이 수행된다. 우선 아래와 같이 N개의 토큰이 주어졌다고 가정해 보자.

$$
(t_1, t_2, \cdots, t_N)
$$

여기서 정방향 언어 모델링을 통한 토큰 시퀀스 확률 값은 다음과 같다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N p(t_k | t_1, t_2, \cdots, t_{k-1})
$$

마찬가지로 역방향 언어 모델링을 통한 토큰 시퀀스 확률 값도 계산할 수 있다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=N}^1 p(t_k | t_{k+1}, t_{k+2}, \cdots, t_{N})
$$

이러한 언어 모델링을 통해서 목적 함수는 Maximum Likelihood Estimation에 의하여 다음과 같이 정리될 수 있다:

$$
\sum_{k=1}^N \left( \log p(t_k | t_1, \cdots, t_{k-1} ; \boldsymbol{\theta}) + \log p(t_k | t_{k + 1}, \cdots, t_N ; \boldsymbol{\theta}) \right)
$$

또한 ELMo는 LSTM 계층을 여럿 쌓은 스택에서 마지막 계층의 은닉 상태(Hidden State) 벡터만을 특징으로 사용하였던 기존의 접근과는 다르게 스택의 모든 계층의 으닉 상태 벡터를 활용하여 특징화를 수행한다. 이는 더 깊게 문맥 정보를 활용할 수 있다는 장점이 있다고 한다.

ELMo의 LSTM 계층이 L층으로 구성되어 있는 경우 토큰 $$t_k$$에 대한 표현 집합은 다음과 같다:

$$
R_k = \left\{ \mathbf{x}_k, \overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j} | j=1,2, \cdots, L \right\}
$$

여기서 $$\mathbf{x}_k$$는 토큰 $$t_k$$의 문맥에 독립적인 표현이다. 그리고 $$\overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j}$$는 각각 $$j$$번째 정방향/역방향 은닉 상태 벡터를 의미한다. 표현 집합은 이어 붙어서(Concatenation) 하나의 벡터로 형성되어 표현 집합을 새롭게 정의하게 된다.

$$
R_k = \left\{ \mathbf{h}_{k, j} | j=1,2, \cdots, L \right\}
$$

ELMo의 최종 표현은 이 표현 집합에 포함된 L개의 표현을 가중합을 통해 계산되어 Task-Specific한 계층의 입력에 활용되어 우리가 원하는 태스크에 활용된다.

이러한 과정은 ELMo의 모델 구조에서도 확인할 수 있다. LSTM 스택 2개가 각각 정방향과 역방향으로 주어진 토큰의 임베딩 시퀀스 $$(E_1, E_2, \cdots, E_N)$$을 은닉 벡터 시퀀스로 인코딩을 수행하고 이 결과를 최종 은닉 벡터 시퀀스 $$(T_1, T_2, \cdots, T_N)$$으로 결합하여 구성한다.

### OpenAI GPT
그 다음으로는 비지도 파인 튜닝(Fine-Tunning) 접근이 있다. 파인 튜닝이란 사전 학습된 모델의 가중치에 Task-Specific한 새로운 데이터를 사전 학습용 데이터에 비해서 상대적으로 소량 활용하여 Task-Specific한 모델 가중치를 획득하는 작업을 말한다.

초기 연구들은 비지도 파인 튜닝 접근을 위하여 사전 학습된 단어(토큰) 임베딩을 활용하는 정도였다. 반면 최근 연구들은 비지도 방식을 통해서 문장, 또는 문서에 대한 임베딩 모델에 대해 사전 학습을 시도하고 있다. 이러한 접근의 좋은 점은 매우 적은 파라미터만이 업데이트되는 정도라서 파인 튜닝 과정이 매우 짧다는 점이다.

`OpenAI GPT`는 이러한 문장 수준의 사전 학습을 통해서 GLUE 벤치마크에서 다양한 문장 수준의 태스크에 대해 SOTA를 달성하였다.

![OpenAI GPT](assets/img/2023-03-07-nlp-04/gpt.png)

`GPT`(Generative Pretrained Transformers)는 이름에서 알 수 있듯 Transformer 구조로 되어 있다. GPT의 컨셉은 기본적으로 준지도 학습(Semi-Supervised Learning)이다. 이는 비지도 사전 학습과 지도 학습 기반의 지도 파인 튜닝을 활용하기 때문이다.

GPT의 `비지도 사전 학습`은 다음과 같은 목적 함수를 최적화하는 방식으로 진행된다. 우선 주어진 레이블이 없는 토큰들의 말뭉치 $$\mathcal{U} = (u_1, u_2, \cdots, u_n)$$에 대하여 다음의 Likelihood Function을 목적 함수로 한다:

$$
\sum_i \log p(u_i | u_{i - k}, \cdots, u_{i - 1}; \boldsymbol{\theta})
$$

여기서 k는 문맥 윈도우 크기이다. 목적 함수의 조건부 확률은 언어 모델을 통해서 계산된다. GPT 논문의 저자들은 이 조건부 확률 계산을 위하여 다층 Transformer 디코더를 사용하였다고 한다.

그 다음으로 GPT의 `지도 파인 튜닝`은 다음과 같이 진행된다. 우선 주어진 레이링된 데이터싯 $$\mathcal{C}$$를 가정하자. 이 데이터셋의 각 인스턴스는 입력 토큰 시퀀스 $$(x_1, x_2, \cdots, x_m)$$과 레이블 $$y$$로 구성된다.

Transformer 디코더의 최종 출력 $$\mathbf{h}$$은 이후 Task-Specific 추가 선형 계층에 입력되어 최종적으로 레이블 $$y$$를 다음과 같이 예측한다:

$$
p(y|x_1, \cdots, x_m) = \text{softmax}(\mathbf{hW}_y)
$$

이는 Maximum Likelihood Estimation을 통해 다음과 같은 목적 함수 계산에 활용된다:

$$
\sum_{\mathbf{x}, y} \log p(y | x_1, \cdots, x_m)
$$

### 지도 데이터를 통한 전이 학습
지도 데이터(Supervised Data)를 통한 `전이 학습`(Transfer Learning) 과정은 위에서 설명하였던 ELMo 및 GPT에서도 Task-Specific한 계층의 학습을 위하여 활용되어 왔다. 이런 방식을 통해 거대 말뭉치로부터 사전 학습된 모델을 활용하여 적은 학습 비용을 바탕으로 파인 튜닝을 수행하여 자연어 추론이나 기계 번역 등의 새로운 태스크에 대해 좋은 성능을 낼 수 있는 모델을 전이 학습을 시킬 수 있다.

이러한 전이 컴퓨터 비전 분야에서 증명되어 왔다. 이미지넷(ImageNet) 등과 같은 거대 데이터를 통해 사전 학습된 모델에 파인 튜닝을 통한 전이 학습은 그 학습 방법이 유효하다는 것을 성능으로 확인할 수 있다.

## BERT 소개
`BERT`(Bidirection Encoder Representations from Transformers)는 구글에서 2018년에 발표한 거대 언어 모델이다. 이름에서 알 수 있듯 BERT는 `Transformer` 기반의 인코더 구조를 지닌 언어 모델이며 따라서 토큰, 또는 문장, 더 나아가 문서의 은닉 표현(Hidden Representation)을 학습하는 모델이라고 볼 수 있다.

![BERT](assets/img/2023-03-07-nlp-04/bert.png)

위 그림은 BERT의 구조를 간단히 설명하는 그림이다. 위 그림에서 확인할 수 있듯이 기본적으로 Transformer로 구성되어 있음을 확인할 수 있으며 Transformer의 특별한 구조를 활용하여 양방향으로의 정보를 활용하여 각 토큰의 인코딩을 수행하는 것을 볼 수 있다.

BERT는 기존 거대 언어 모델들과 마찬가지로 레이블이 없는 말뭉치를 통해 사전 학습을 수행하고 이렇게 사전 학습이 된 모델을 바탕으로 사용자가 원하는 특정 태스크에 대해 파인 튜닝을 수행하여 최종적인 태스크에 특화된 모델을 획득하는 과정을 거친다. BERT는 이름에서 알 수 있듯 기본적으로 입력 문장에 대해서 인코딩을 수행하여 은닉 벡터 표현을 획득하고 이렇게 획득한 벡터 표현을 바탕으로 태스크에 특화된 새로운 인공 신경망 계층을 추가하여 파인 튜닝을 수행한다.

### BERT의 구조
BERT의 구조는 위에서 말했듯이 다층 양방향 Transformer 인코더 형태이다. BERT의 저자들은 이 구조를 구성하는 Transformer는 원래의 Transformer에서 변화없이 그대로 사용하였다고 한다. Transformer에 대해서 더 자세히 알아보고 싶으면 이전 [포스트](https://hcnoh.github.io/2023-02-25-nlp-03)를 참고하면 된다.

저자들은 BERT 모델을 크기별로 크게 2개를 제안했다. 먼저 `BERT BASE`이다. BERT BASE 말 그대로 기본 BERT이며 Transformer 스택 계층 수 $$L$$을 12로, 은닉 표현 사이즈 $$H$$를 768로, Self-Attention Head 수 $$A$$를 12로 설정한 모델이다.

그 다음으로는 `BERT LARGE`가 있다. BERT LARGE는 $$L$$을 24로, $$H$$를 1024로, $$A$$를 16으로 설정한 모델이다. 아래 표에 두 모델에 대해서 간단히 정리를 해보았다.

|            | L  | H    | A  | 파라미터 수 |
|--          |--  |--    |--  |--        |
| BERT BASE  | 12 | 768  | 12 | 110M     |
| BERT LARGE | 24 | 1024 | 16 | 340M     | 

BERT BASE에 비해서 BERT LARGE는 파라미터 수가 약 3배 정도 되는 340만 정도이다. 원래 저자들이 의도한 모델은 BERT LARGE이지만 BERT BASE는 GPT와 비교를 하기 위하여 만든 모델이라는 듯 하다.

### BERT의 입/출력 표현
BERT의 입/출력 표현 방법은 다양한 NLP 태스크를 수행하기 위한 형태로 설계되었다. 단순히 토큰 표현들 뿐 아니라 단일 문장, 그리고 질의 응답 태스크 등을 수행하기 위한 이중 문장을 입력받을 수 있는 형태이다.

우선 모든 입력 토큰은 `WordPiece`라는 이름의 임베딩을 사용했다. WordPiece에 대해서는 나중에 자세히 다뤄보기로 한다. 어쨌든 모든 토큰들은 이 WordPiece 임베딩을 통해서 벡터 형태로 변환된다. 또한 추가적으로 모든 문장의 첫 번째 토큰은 [CLS] 토큰이라는 것을 붙여서 사용한다. 이 [CLS] 토큰은 문장 전체의 인코딩을 위한 토큰으로 문장 분류 등의 태스크에 활용될 수 있다.

이중 문장이 토큰 시퀀스로 입력되어 모델에 들어오는 경우 이 문장들을 구분하는 방법이 필요하다. BERT에서는 두 가지 방법을 통해서 이 구분을 수행한다. 먼저 구분자인 [SEP] 토큰을 중간에 껴넣어서 구분하는 방법이다. 두 번째는 각 토큰 임베딩에 추가로 문장 A에 속한 것인지 문장 B에 속한 것인지를 나타내는 임베딩 벡터를 이어 붙여서 모델이 구분할 수 있도록 하는 방법이다.

![BERT 입력 표현 방법](assets/img/2023-03-07-nlp-04/bert-input-representation.png)

위 그림은 BERT의 입력 표현 방법을 나타내는 그림이다. [SEP] 토큰을 통해 두 문장이 구분되고 있으며 또한 Segment 임베딩을 통해서 각각의 토큰이 첫 번째 문장(A)에 속한 것인지 두 번째 문장(B)에 속한 것인지를 나타내고 있다. 물론 맨 앞에는 [CLS] 토큰을 통해서 입력 문장이 시작됨을 나타내고 있다. 마지막으로 각 토큰들에 대한 위치 임베딩까지 더해진다. 이 세 가지의 임베딩 벡터들의 합을 통해서 각 토큰의 표현 벡터를 계산한다.

## BERT의 사전 학습
기존의 언어 모델들과 다르게 BERT는 정방향 또는 역방향의 사전 학습을 수행하지 않는다. 대신 두 가지의 비지도 태스크를 통해서 사전 학습이 진행된다. 두 가지 태스크는 각각 `마스크 언어 모델`(Masked LM, MLM)과 `다음 문장 예측`(Next Sentence Prediction, NSP)이다. BERT의 사전 학습 과정은 두 가지 사전 학습 태스크에 대해서 태스크에 특화된 추가 인공 신경망 계층을 학습하는 방식으로 진행된다.

![BERT의 사전 학습 및 파인 튜닝](assets/img/2023-03-07-nlp-04/bert-pretraining-and-finetuning.png)

### 태스크 (1) - 마스크 언어 모델
BERT는 기존의 정방향/역방향 모델들과는 다른 진정한 의미에서의 양방향 모델이라고 저자들은 주장한다. BERT와는 다르게 기존의 양방향 모델이라고 여겨지던 이전 모델들은 단순히 정방향/역방향의 표현을 이어 붙이는 정도의 얕은 수준의 양방향 모델이었고 진정한 의미의 양방향 모델은 BERT 뿐이라는 것이다.

이러한 BERT의 양방향성을 잘 활용하여 사전 학습을 수행하기 위해서 단순하게 입력 토큰 시퀀스에서 임의로 선택된 토큰들을 마스킹하여 BERT 모델로 하여금 마스킹된 토큰이 무엇인지 예측하는 방식의 사전 학습을 제안한다. 이러한 방식의 사전 학습을 마스크 언어 모델이라고 한다.

저자들은 마스크 언어 모델 방식을 통해 사전 학습을 진행하기 위해서 다음과 같은 방식의 마스킹 방법을 활용했다. 훈련용 데이터를 생성하는 과정에서 전체 토큰들 중 15%의 토큰을 랜덤하게 추출하여 마스킹에 활용한다. 이렇게 추출된 토큰들 각각은 80%는 [MASK] 토큰으로, 10%는 임의의 다른 토큰으로, 그리고 마지막 10%는 변화없이 그대로 둔다.

이렇게 마스킹된 말뭉치를 바탕으로 BERT는 사전 학습이 진행된다. 말뭉치로부터 추출된 문장의 토큰 시퀀스를 입력받은 모델은 각 토큰 시퀀스에 대한 표현 벡터를 생성하고 이를 통해서 마스크 언어 모델 태스크를 수행한다. 이는 마스크 언어 모델 태스크에 특화된 인공 신경망 모델 계층을 추가하여 진행된다.

### 태스크 (2) - 다음 문장 예측
마스크 언어 모델이 토큰 단위의 양방향 언어 모델링을 수행하지만 이것만으로는 한계가 있다. 많은 자연어 처리 태스크가 단순히 토큰 단위의 태스크를 넘어서 문장 단위의 이해가 필요한 태스크이기 때문이다. 질의 응답(Question Answering, QA) 또는 자연어 추론(Natural Language Inference, NLI)이 그 예시이다. 이러한 태스크들은 주어진 두 문장 사이의 관계를 이해해야만 하는 태스크들이다.

이러한 관계를 학습시키기 위하여 BERT는 사전 학습 과정에서 다음 문장 예측 태스크를 수행한다. 다음 문장 예측은 다음과 같은 과정을 통해서 진행된다. 우선 훈련용 데이터셋으로부터 문장 2개를 추출한다. 두 문장은 각각 문장 A, B로 지칭한다. 문장 A, B를 추출하는 과정에서 50%는 문장 B가 실제로 문장 A 다음에 등장하는 문장으로, 나머지 50%는 문장 A와 관계없는 문장으로 추출한다.

문장 B가 실제로 문장 A 다음에 추출된 경우에 레이블은 IsNext, 그렇지 않은 경우에는 NotIsNext로 레이블을 설정하여 이를 예측하도록 하는 태스크 특화된 인공 신경망 계층을 추가하여 사전 학습을 수행한다. 사전 학습을 위한 손실 함수의 계산은 마스크 언어 모델을 위한 손실 함수와 다음 문장 예측을 위한 손실 함수의 평균으로 계산한다.

### 사전 학습 데이터

## BERT의 파인 튜닝


## 참고 자료
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
- [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised)

## 수정 사항
- 2023.03.07
    - 최초 게제