---
layout: post
use_math: true
title: "[Natural Language Processing] 4. BERT"
date: 2023-03-07 00:00:00
tagline: "자연어 처리에서 최근 가장 많이 활용되는 거대 언어 모델의 시초인 BERT에 대해서 정리"
categories:
- Natural Language Processing Study
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2023-03-07-nlp-04
---

이번 포스트에서는 최근 자연어 처리 분야에서 가장 많이 활용되고 있는 거대 언어 모델의 아이디어를 처음으로 제안했단 `BERT`에 대해서 다룬다. BERT는 등장하자마자 획기적이라는 평가를 받았던 언어 모델이며 기존에 존재했던 대부분의 자연어 처리 태스크에서 `SOTA`(State-of-the-Art) 수준을 달성하는 강력한 모습을 보였었다. BERT의 구조와 훈련 방식을 따라가는 구성으로 이번 포스트의 내용을 다뤄볼 계획이다.

## 목차
- [기존 언어 모델 소개](#기존-언어-모델-소개)
    - [ELMo](#elmo)
    - [OpenAI GPT](#openai-gpt)

## 기존 언어 모델 소개
거대 언어 모델 연구는 지금까지 언어 모델에 대한 `사전 학습`(Pre-Training)을 바탕으로 토큰 표현(Representation)을 잘 획득하고 이를 통해 다양한 자연어 처리 태스크를 수행하는 것을 목적으로 수행되어 왔다. 이러한 방향의 언어 모델 연구 역사는 꽤나 오래되었지만 크게 3가지 공통된 방향성이 존재한다.

### ELMo
우선 비지도 특징 기반 접근이 있다. 이 접근은 비지도 기반 특징화를 통해 단어 또는 토큰의 좋은 표현을 획득하려고 했던 연구들을 말한다. 일반적으로는 `정방향`(Left-to-Right) 언어 모델링 기반 목적 함수 최적화를 통해 단어/토큰 표현을 획득했었고 이런 방식은 정방향 언어 모델링은 문장 임베딩(Sentence Embedding), 더 나아가 문단 임베딩(Paragraph Embedding)으로 일반화되어 왔다.

`ELMo`(Embeddings from Language Models)는 이러한 비지도 특징 기반 접근의 대표적인 언어 모델이다. 기존의 연구들과는 다르게 ELMo는 문장 내에서 최대한 문맥 정보를 추출할 수 있는 방법을 통해 단어 표현을 고도화하고자 하였다. 우선 정방향 언어 모델링 뿐 아니라 `역방향`(Right-to-Left) 언어 모델링을 통한 정보를 둘다 활용하여 `양방향`(Bidirectorional) 언어 모델링을 구현하고자 하였다.

![ELMo](assets/img/2023-03-07-nlp-04/elmo.png)

목적 함수 계산을 위한 ELMo의 양방향 언어 모델링은 다음과 같이 수행된다. 우선 아래와 같이 N개의 토큰이 주어졌다고 가정해 보자.

$$
(t_1, t_2, \cdots, t_N)
$$

여기서 정방향 언어 모델링을 통한 토큰 시퀀스 확률 값은 다음과 같다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N p(t_k | t_1, t_2, \cdots, t_{k-1})
$$

마찬가지로 역방향 언어 모델링을 통한 토큰 시퀀스 확률 값도 계산할 수 있다:

$$
p(t_1, t_2, \cdots, t_N) = \prod_{k=N}^1 p(t_k | t_{k+1}, t_{k+2}, \cdots, t_{N})
$$

이러한 언어 모델링을 통해서 목적 함수는 Maximum Likelihood Estimation에 의하여 다음과 같이 정리될 수 있다:

$$
\sum_{k=1}^N \left( \log p(t_k | t_1, \cdots, t_{k-1} ; \boldsymbol{\theta}) + \log p(t_k | t_{k + 1}, \cdots, t_N ; \boldsymbol{\theta}) \right)
$$

또한 ELMo는 LSTM 계층을 여럿 쌓은 스택에서 마지막 계층의 은닉 상태(Hidden State) 벡터만을 특징으로 사용하였던 기존의 접근과는 다르게 스택의 모든 계층의 으닉 상태 벡터를 활용하여 특징화를 수행한다. 이는 더 깊게 문맥 정보를 활용할 수 있다는 장점이 있다고 한다.

ELMo의 LSTM 계층이 L층으로 구성되어 있는 경우 토큰 $$t_k$$에 대한 표현 집합은 다음과 같다:

$$
R_k = \left\{ \mathbf{x}_k, \overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j} | j=1,2, \cdots, L \right\}
$$

여기서 $$\mathbf{x}_k$$는 토큰 $$t_k$$의 문맥에 독립적인 표현이다. 그리고 $$\overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j}$$는 각각 $$j$$번째 정방향/역방향 은닉 상태 벡터를 의미한다. 표현 집합은 이어 붙어서(Concatenation) 하나의 벡터로 형성되어 표현 집합을 새롭게 정의하게 된다.

$$
R_k = \left\{ \mathbf{h}_{k, j} | j=1,2, \cdots, L \right\}
$$

ELMo의 최종 표현은 이 표현 집합에 포함된 L개의 표현을 가중합을 통해 계산되어 Task-Specific한 계층의 입력에 활용되어 우리가 원하는 태스크에 활용된다.

### OpenAI GPT
그 다음으로는 비지도 미세 조정(Fine-Tunning) 접근이 있다. 미세 조정이란 사전 학습된 모델의 가중치에 Task-Specific한 새로운 데이터를 사전 학습용 데이터에 비해서 상대적으로 소량 활용하여 Task-Specific한 모델 가중치를 획득하는 작업을 말한다.

초기 연구들은 비지도 미세 조정 접근을 위하여 사전 학습된 단어(토큰) 임베딩을 활용하는 정도였다. 반면 최근 연구들은 비지도 방식을 통해서 문장, 또는 문서에 대한 임베딩 모델에 대해 사전 학습을 시도하고 있다. 이러한 접근의 좋은 점은 매우 적은 매개변수만이 갱신이 되는 정도라서 미세 조정 과정이 매우 짧다는 점이다.

`OpenAI GPT`는 이러한 문장 수준의 사전 학습을 통해서 GLUE 벤치마크에서 다양한 문장 수준의 태스크에 대해 SOTA를 달성하였다.

![OpenAI GPT](assets/img/2023-03-07-nlp-04/gpt.png)

`GPT`(Generative Pretrained Transformers)는 이름에서 알 수 있듯 Transformer 구조로 되어 있다. GPT의 컨셉은 기본적으로 준지도 학습(Semi-Supervised Learning)이다. 이는 비지도 사전 학습과 지도 학습 기반의 지도 미세 조정을 활용하기 때문이다.

GPT의 `비지도 사전 학습`은 다음과 같은 목적 함수를 최적화하는 방식으로 진행된다. 우선 주어진 레이블이 없는 토큰들의 말뭉치 $$\mathcal{U} = (u_1, u_2, \cdots, u_n)$$에 대하여 다음의 Likelihood Function을 목적 함수로 한다:

$$
\sum_i \log p(u_i | u_{i - k}, \cdots, u_{i - 1}; \boldsymbol{\theta})
$$

여기서 k는 문맥 윈도우 크기이다. 목적 함수의 조건부 확률은 언어 모델을 통해서 계산된다. GPT 논문의 저자들은 이 조건부 확률 계산을 위하여 다층 Transformer 디코더를 사용하였다고 한다.

그 다음으로 GPT의 `지도 미세 조정`은 다음과 같이 진행된다. 우선 주어진 레이링된 데이터싯 $$\mathcal{C}$$를 가정하자. 이 데이터셋의 각 인스턴스는 입력 토큰 시퀀스 $$(x_1, x_2, \cdots, x_m)$$과 레이블 $$y$$로 구성된다.

Transformer 디코더의 최종 출력 $$\mathbf{h}$$은 이후 Task-Specific 추가 선형 계층에 입력되어 최종적으로 레이블 $$y$$를 다음과 같이 예측한다:

$$
p(y|x_1, \cdots, x_m) = \text{softmax}(\mathbf{hW}_y)
$$

이는 Maximum Likelihood Estimation을 통해 다음과 같은 목적 함수 계산에 활용된다:

$$
\sum_{\mathbf{x}, y} \log p(y | x_1, \cdots, x_m)
$$

![](assets/img/2023-03-07-nlp-04/bert.png)

## 참고 자료
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)

## 수정 사항
- 2023.03.07
    - 최초 게제