---
layout: post
use_math: true
title: "[자연어 처리] 6. LLaMA"
date: 2024-02-15 00:00:00
tagline: "Meta에서 개발하고 공개한 생성형 거대 언어 모델인 LLaMA에 대해서 정리"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-02-15-nlp-06
---

이번 포스트에서는 최근 생성형 언어 모델 연구의 트렌드를 이끌고 있는 연구들 중에서 Meta에서 개발하고 공개까지 한 모델인 `LLaMA`에 대해서 다룬다. LLaMA는 GPT-3에 비해서 훨씬 더 적은 모델 크기를 보유하고도 GPT-3를 능가하는 벤치마크 성능을 보인다. 또한 LLaMA의 소형 버전은 V100 기준 하나의 GPU 만으로도 추론이 가능할 정도로 경량화가 잘 되어 있다. 이번 포스트는 LLaMA와 LLaMA-2 모델들을 살펴보고 그 특징이 어떤지 정리해보도록 한다.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## LLaMA 소개
`LLaMA`(Large Language Model Meta AI)는 Meta에서 개발하고 공개한 생성형 `기초 언어 모델`(Foundation Language Models)이다. 기초 언어 모델이라고 불리우는 이유는 그 경량성에 있다. LLaMA는 파라미터의 수가 70억인 모델부터 650억인 모델까지 있는데 70억 파라미터의 모델은 파라미터 수가 1750억인 GPT-3에 비해서 무려 10%도 되지 않는다.

기존 언어 모델 연구는 대부분 그 규모를 확장시키는 방식으로 진행되어 왔다. 모델 크기를 늘리고 거기에 걸맞게 데이터의 크기를 늘리는 방식으로 말이다. 하지만 LLaMA는 이러한 규모 확장 방식에 반하는 소규모의 모델을 지향하여 V100 기준 하나의 GPU에서도 추론이 가능한 경량성을 달성하였다. 이 뿐만 아니라 LLaMA는 기존의 GPT-3나 구글의 PaLM 등과는 다르게 공개된 데이터만을 가지고 사전 훈련을 수행하였다. 저자들은 LLaMA의 이러한 경량성이 AI 연구의 민주화를 이끌고 있다고 주장한다.

또한 Meta는 곧바로 LLaMA의 개선된 버전인 LLaMA-2를 발표했다. LLaMA-2는 LLaMA에 비해서 사전 훈련을 위한 공개 말뭉치의 크기를 40% 증가시켰고 모델의 문맥 길이를 2배 증가시켰다. 또한 Grouped-Query Attention이라는 것을 적용하였다고 한다. LLaMA-2는 70억, 130억, 700억 파라미터 버전을 제공한다. 또한 여기에 더하여 LLaMA-2의 챗봇 특화 파인 튜닝 버전인 LLaMA-2-Chat도 공개했다.

## 사전 훈련용 데이터셋

## 정리

## 참고 자료
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

## 수정 사항
- 2024.02.15
    - 최초 업로드