---
layout: post
use_math: true
title: "[자연어 처리] 6. LLaMA"
date: 2024-02-15 00:00:00
tagline: "Meta에서 개발하고 공개한 생성형 거대 언어 모델인 LLaMA에 대해서 정리"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-02-15-nlp-06
---

이번 포스트에서는 최근 생성형 언어 모델 연구의 트렌드를 이끌고 있는 연구들 중에서 Meta에서 개발하고 공개까지 한 모델인 `LLaMA`에 대해서 다룬다. LLaMA는 GPT-3에 비해서 훨씬 더 적은 모델 크기를 보유하고도 GPT-3를 능가하는 벤치마크 성능을 보인다. 또한 LLaMA의 소형 버전은 V100 기준 하나의 GPU 만으로도 추론이 가능할 정도로 경량화가 잘 되어 있다. 이번 포스트는 LLaMA와 LLaMA-2 모델들을 살펴보고 그 특징이 어떤지 정리해보도록 한다.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## LLaMA 소개
`LLaMA`(Large Language Model Meta AI)는 Meta에서 개발하고 공개한 생성형 `기초 언어 모델`(Foundation Language Models)이다. 기초 언어 모델이라고 불리우는 이유는 그 경량성에 있다. LLaMA는 파라미터의 수가 70억인 모델부터 650억인 모델까지 있는데 70억 파라미터의 모델은 파라미터 수가 1750억인 GPT-3에 비해서 무려 10%도 되지 않는다.

기존 언어 모델 연구는 대부분 그 규모를 확장시키는 방식으로 진행되어 왔다. 모델 크기를 늘리고 거기에 걸맞게 데이터의 크기를 늘리는 방식으로 말이다. 하지만 LLaMA는 이러한 규모 확장 방식에 반하는 소규모의 모델을 지향하여 V100 기준 하나의 GPU에서도 추론이 가능한 경량성을 달성하였다. 이 뿐만 아니라 LLaMA는 기존의 GPT-3나 구글의 PaLM 등과는 다르게 공개된 데이터만을 가지고 사전 훈련을 수행하였다. 저자들은 LLaMA의 이러한 경량성이 AI 연구의 민주화를 이끌고 있다고 주장한다.

또한 Meta는 곧바로 LLaMA의 개선된 버전인 LLaMA-2를 발표했다. LLaMA-2는 LLaMA에 비해서 사전 훈련을 위한 공개 말뭉치의 크기를 40% 증가시켰고 모델의 문맥 길이를 2배 증가시켰다. 또한 Grouped-Query Attention이라는 것을 적용하였다고 한다. LLaMA-2는 70억, 130억, 700억 파라미터 버전을 제공한다. 또한 여기에 더하여 LLaMA-2의 챗봇 특화 파인 튜닝 버전인 LLaMA-2-Chat도 공개했다.

## 사전 훈련용 데이터셋

Dataset         | Sampling prop.    | Epochs    | Disk size
--              | --                | --        | --
CommonCrawl     | 67.0%             | 1.10      | 3.3 TB
C4              | 15.0%             | 1.06      | 783 GB
Github          | 4.5%              | 0.64      | 328 GB
Wikipedia       | 4.5%              | 2.45      | 83 GB
Books           | 4.5%              | 2.23      | 85 GB
ArXiv           | 2.5%              | 1.06      | 92 GB
StackExchange   | 2.0%              | 1.03      | 78 GB

위 표는 LLaMA의 사전 훈련에 사용한 데이터셋을 정리한 것이다. 사전 훈련용 데이터셋은 다양한 도메인의 데이터 원천들의 조합으로 구성되어 있다.

### 영문 CommonCrawl
저자들은 LLaMA 학습을 위해 2017년부터 2020년까지 5개의 영문 CommonCrawl 덤프를 전처리하여 사전 훈련에 사용하였다. 여기에는 CCNet 파이프라인이라는 것을 사용했는데 이 파이프라인을 통해서 데이터를 줄 수준으로 복제하고 fastText 선형 분류기를 통해 언어를 식별하여 영문이 아닌 페이지는 제거하였다. 또한 CCNet 파이프라인은 n그램 언어 모델을 통해 낮은 퀄리티의 페이지들도 제거할 수 있다. 또한 저자들은 선형 분류기를 훈련시켜 위키피디아의 레퍼런스로 사용된 문서와 랜덤하게 추출된 일반 문서를 분류하여 필터링에 사용할 수 있도록 하였다.

### C4
저자들은 또한 공개된 C4 데이터셋을 활용하였다. CCNet 파이프라인과 거의 유사한 방식으로 전처리가 되었으며 유일한 차이점은 CCNet의 경우 선형 분류기를 통해 퀄리티 필터링을 수행하였던 반면 C4 데이터셋 전처리에서는 웹페이지 내의 문장 부호(Punctuation Marks)의 유무, 단어나 문장의 갯수 등의 휴리스틱을 활용하였다는 점이다.

## LLaMA의 모델 구조
LLaMA는 기존의 Transformer 기반의 모델에서 최근에 제안된 다양한 방법들을 활용하여 모델 구조의 개선이 있었다. 이 개선점은 `Pre-Normalization`, `SwiGLU 활성 함수`, `Rotary 임베딩`의 3가지가 있다.

### Pre-Normalization
저자들은 LLaMA의 훈련 안정성 향상을 위하여 각 Transformer 계층의 입력을 Normalization하였다. 이는 기존의 출력을 Normalization하는 것과는 다르다. 여기에는 RMSNorm Normalization 함수를 사용하였다고 한다.

### SwiGLU 활성 함수

### Rotary 임베딩

## 정리

## 참고 자료
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

## 수정 사항
- 2024.02.15
    - 최초 업로드