---
layout: post
use_math: true
title: "[자연어 처리] 5. GPT"
date: 2024-01-05 00:00:00
tagline: "자연어 처리에서 생성형 거대 언어 모델의 대표격인 GPT에 대한 소개"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-05-nlp-05
---

이번 포스트에서는 최근의 생성형 AI의 트렌드를 이끌고 있는 생성형 거대 언어 모델의 대표 주자인 `GPT`에 대해서 다룬다. GPT는 현재 세계에서 가장 인기가 많고 가장 많이 활용되는 생성형 모델이다. 이러한 GPT의 인기를 이끄는 것은 역시 사람 수준의 언어 이해력 및 구사력일 것이다. 이번 포스트를 통해서 GPT는 어떤 구조를 가진 모델이고 어떤 방식으로 훈련이 되었는지, 그리고 어떻게 활용될 수 있을지를 다뤄보고자 한다.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## GPT 모델 구조
GPT는 기본적으로 사람의 언어 구사를 흉내내도록 학습된 딥러닝 모델이라고 볼 수 있다. 이를 위하여 GPT는 기본적으로 짧은 입력 텍스트를 받아서 더욱 많은 양의 텍스트를 생성하도록 구성되어 있다. 기본적으로 GPT는 `Autoregressive Model` 모델이다. 즉, 자기 자신이 생성한 텍스트를 다시 자신의 입력으로 받아 이를 바탕으로 다시 출력 텍스트를 생성하는 자기회귀적인 특성을 지닌다.

아래는 GPT의 정의를 정리한 것이다:

**Definition)**
- GPT는 거대한 양의 텍스트 데이터를 통해서 사전 훈련(Pre-trained)된 언어 모델이며 다양한 분야의 언어 태스크를 수행할 수 있다.
- GPT는 딥러닝을 통해 주어진 텍스트 기반의 입력에 대해서 사람과 유사한 텍스트를 생성할 수 있는 언어 모델이다.
- GPT는 OpenAI에서 지능형 시스템을 위해 개발한 언어 모델이며 ChatGPT 프로젝트에 활용되었다.

## GPT와 BERT의 차이

## 정리


## 참고 자료
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=transaction)
- [Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions](https://arxiv.org/abs/2305.10435)

## 수정 사항
- 2024.01.02
    - 최초 게제