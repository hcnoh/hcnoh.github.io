---
layout: post
use_math: true
title: "[자연어 처리] 5. GPT"
date: 2024-01-05 00:00:00
tagline: "자연어 처리에서 생성형 거대 언어 모델의 대표격인 GPT에 대한 소개"
categories:
- 자연어 처리 스터디
tags:
- deep learning
- natural language processing
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-05-nlp-05
---

이번 포스트에서는 최근의 생성형 AI의 트렌드를 이끌고 있는 생성형 거대 언어 모델의 대표 주자인 `GPT`에 대해서 다룬다. GPT는 현재 세계에서 가장 인기가 많고 가장 많이 활용되는 생성형 모델이다. 이러한 GPT의 인기를 이끄는 것은 역시 사람 수준의 언어 이해력 및 구사력일 것이다. 이번 포스트를 통해서 GPT는 어떤 구조를 가진 모델이고 어떤 방식으로 훈련이 되었는지, 그리고 어떻게 활용될 수 있을지를 다뤄보고자 한다.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## GPT 모델 구조
GPT는 기본적으로 사람의 언어 구사를 흉내내도록 학습된 딥러닝 모델이라고 볼 수 있다. 이를 위하여 GPT는 짧은 입력 텍스트를 받아서 더욱 많은 양의 텍스트를 생성하도록 구성되어 있다. 기본적으로 GPT는 `Autoregressive Model` 모델이다. 즉, 자기 자신이 생성한 텍스트를 다시 자신의 입력으로 받아 이를 바탕으로 다시 출력 텍스트를 생성하는 자기회귀적인 특성을 지닌다.

아래는 GPT의 정의를 정리한 것이다:

**Definition)**
- GPT는 거대한 양의 텍스트 데이터를 통해서 사전 훈련(Pre-trained)된 언어 모델이며 다양한 분야의 언어 태스크를 수행할 수 있다.
- GPT는 딥러닝을 통해 주어진 텍스트 기반의 입력에 대해서 사람과 유사한 텍스트를 생성할 수 있는 언어 모델이다.
- GPT는 OpenAI에서 지능형 시스템을 위해 개발한 언어 모델이며 ChatGPT 프로젝트에 활용되었다.

GPT는 자연어 처리의 다양한 태스크들, 예를 들면 언어 모델링, 텍스트 분류, 텍스트 생성 등의 태스크를 수행하기 위한 신경망 구조로 되어있다. GPT 모델의 구조는 기본적으로 Transformer 모델을 기반으로 한다.

![](assets/img/2024-01-05-nlp-05/transformer.png){: width="50%" height="50%"}

위 그림은 기본적인 Transformer 구조이다. Transformer에 대한 자세한 내용은 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)를 참고하면 좋다. Transformer는 인코더와 디코더로 구성되어 있다. GPT는 여기서 Transformer 디코더 구조만으로 구성되어 있다.

![](assets/img/2024-01-05-nlp-05/transformer-decoder.png){: width="50%" height="50%"}

위 그림은 Transformer 구조에서 디코더만을 나타낸 것이다. Transformer 구조에서 인코더와 디코더의 가장 큰 구조적인 차이는 `Multi-Head Attention` 메커니즘의 입력이 마스킹이 되어있는지 아닌지로 구분할 수 있을 것이다. 마스킹이 되어있는 경우를 `Masked Multi-Head Attention`이라고 한다. Masked Multi-Head Attention은 토큰 시퀀스의 Self Attention 과정에서 현재 토큰의 시점보다 미래 시점의 토큰들은 마스킹을 하여 미래 시점의 토큰을 예측하도록 학습을 수행하기 위한 구조를 가지고 있다.

GPT에서 사용된 Transformer의 디코더는 인코더 시퀀스에 대한 Attention을 수행할 필요가 없기 때문에 위 그림에서 중간 부분에 있는 Multi-Head Attention 메커니즘 계층을 제거하였다고 한다.

![](assets/img/2024-01-05-nlp-05/gpt-arch.png)

위 그림은 전반적인 GPT의 구조와 입력 방식을 설명하고 있다. 그림에서 확인할 수 있듯 GPT는 Transformer 디코더 구조를 약간 변형하여 활용하고 있으며 수행하려는 NLP 태스크에 따라서 텍스트 입력 방식이 어떻게 변화되는지 알 수 있다.

## GPT와 BERT의 차이
GPT가 먼저 발표된 모델이지만 그 직후 바로 BERT 모델이 발표되면서 두 모델은 필연적으로 비교가 될 수 밖에 없다. 하지만 두 모델은 근본적으로 그 역할이 다르며 그에 따른 구조도 Transformer를 활용한다는 점을 제외하고는 매우 다르다.

우선 GPT는 위에서 설명하였듯 Transformer의 디코더 구조를 기반으로 하고 있다. 따라서 입력 토큰 시퀀스에 대해서 미래 시점은 확인하지 않고 과거 시점들에 대해서만 Attention을 수행하여 현재 시점에 대한 결과물을 출력한다. 반면 BERT는 이름에서 확인할 수 있듯 인코더의 역할을 하는 모델이다. 이에 따라 BERT는 Transformer 인코더 구조를 기반으로 하는 모델 구조를 가지고 있다.

![](assets/img/2024-01-05-nlp-05/transformer-encoder.png){: width="50%" height="50%"}

위 그림은 Transformer의 인코더 구조를 나타내는 그림이다. 인코더는 디코더와 다르게 입력 토큰 시퀀스에서 현재 시점의 토큰에 대해 인코딩을 하는 경우 과거와 미래 시점의 코든 토큰의 시퀀스를 Attention에 활용한다. 즉 양방향으로 정보를 활용하여 현재 시점의 토큰을 인코딩하는 것이다.

## GPT 프레임워크
GPT는 위에서 설명하였듯 Transformer 디코더 기반의 Autoregressive 모델이다. 입력 토큰 시퀀스 및 모델 자신이 생성한 모든 시점의 시퀀스를 다시 모델이 입력받아 그 다음 토큰을 출력하려고 한다. 이런 방식으로 GPT는 사람의 언어 구사를 흉내내도록 학습되어진다.

### 비지도 사전 훈련
사람의 언어 구사를 흉내내도록 학습시키기 위하여 GPT는 사람이 생성한 말뭉치 데이터의 언어 모델링을 수행하도록 학습된다. 주어진 비지도 말뭉치의 토큰들을 다음과 같이 정의하자:

$$
\mathcal{U} = \{ u_1, \cdots, u_n \}.
$$

다음은 GPT의 비지도 사전 훈련에 사용한 표준적인 언어 모델링 목적 함수이다. 우리가 잘 알고 있듯이 Likelihood 함수 형태이다:

$$
L_1(\mathcal{U}) = \sum_i \log p\left( u_i | u_{i - k}, \cdots, u_{i - 1} \right).
$$

여기서 $$k$$는 문맥 윈도우의 크기이다. 즉, 이 언어 모델링은 다음 토큰을 예측할 때 $$k$$개의 이전 토큰들을 참고한다는 의미이다. 여기서 확률 분포 $$p$$는 GPT의 파라미터 $$\boldsymbol{\theta}$$로 파라미터화될 것이다:

$$
L_1(\mathcal{U}) = \sum_i \log p\left( u_i | u_{i - k}, \cdots, u_{i - 1} ; \boldsymbol{\theta} \right).
$$

위에서 설명하였듯 GPT는 다층 Transformer 디코더로 구성되어 언어 모델링을 수행한다. 이 모델은 입력 문맥 토큰들에 Multi-Head Attention을 수행하고 이 결과물을 위치별 완전 연결 순전파 망을 활용하여 타겟 토큰에 대한 출력 분포를 생성한다. 더 정확하게는 아래와 같은 식을 통해서 언어 모델링이 수행된다:

$$
\begin{array}{rl}
\mathbf{h}_0 & = \mathbf{u} \mathbf{W}_e + \mathbf{W}_p. \\
\mathbf{h}_l & = \text{transformer-block}(\mathbf{h}_{l - 1}), \ \forall i \in [1, n]. \\
p(u) & = \text{softmax}\left( \mathbf{h}_n \mathbf{W}_e^\text{T} \right).
\end{array}
$$

여기서 벡터 $$\mathbf{u} = (u_{-k}, \cdots, u_{-1})$$는 토큰들의 문맥 벡터이고 $$n$$은 디코더 계층 개수이다. $$\mathbf{W}_e$$는 토큰 임베딩 행렬이고 $$\mathbf{W}_p$$는 위치 임베딩 행렬이다.

## GPT의 발전 과정

### GPT-1

### GPT-2

### GPT-3

![](assets/img/2024-01-05-nlp-05/rlhf.png)

### GPT-3.5 & GPT-4

## 정리


## 참고 자료
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=transaction)
- [Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions](https://arxiv.org/abs/2305.10435)

## 수정 사항
- 2024.01.13
    - 최초 게제