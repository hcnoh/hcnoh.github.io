---
layout: post
use_math: true
title: "[비전-언어 모델] 1. 비전-언어 모델 소개"
date: 2024-01-03 00:00:00
tagline: "최근 가장 많이 사용되고 있는 멀티모달 모델인 비전-언어 모델에 대한 개괄적인 소개 포스트"
categories:
- 비전-언어 모델 스터디
tags:
- deep learning
- natural language processing
- computer vision
- vision-language models
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-03-vlm-01
---

이번 포스트에서는 최근 AI 업계의 트렌드인 멀티모달 모델, 그 중에서도 특히 많이 활용되는 비전-언어 모델에 대한 소개를 다뤄볼 예정이다. 이를 위하여 [링크](https://huggingface.co/blog/vision_language_pretraining)를 번역 및 참고하여 이번 포스트를 작성하였다.

## 목차
- [비전-언어 모델 소개](#비전-언어-모델-소개)
- [학습 전략](#학습-전략)
    - [Contrastive Learning](#constrastive-learning)
    - [PrefixLM](#prefixlm)
    - [Cross Attention을 통한 멀티모달 결합](#cross-attention을-통한-멀티모달-결합)
    - [마스크 언어 모델링 / 이미지-텍스트 매칭](#마스크-언어-모델링--이미지-텍스트-매칭)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## 비전-언어 모델 소개
`비전-언어 모델`(Vision-Language Models)이란 간단하게 비전과 언어의 양상(Modality)를 통합하는 모델을 말한다. 조금 더 정확하게는 이미지와 자연어 텍스트를 모두 처리할 수 있는 능력을 가진 모델을 말할 수 있다. 비전-언어 모델의 이해를 도울 수 있는 간단한 예시를 살펴보도록 하자. 아래의 예시는 비전-언어 모델을 통한 Zero-shot 이미지 분류 예시이다. 우리는 모델에게 이미지와 약간의 프롬프트를 입력하여 모델로 하여금 입력 이미지에 가장 그럴듯한 프롬프트를 고르도록 할 것이다.

![](assets/img/2024-01-03-vlm-01/zero-shot-image-classification.png)

*(출처: [https://huggingface.co/blog/vision_language_pretraining](https://huggingface.co/blog/vision_language_pretraining))*

이 예측을 수행하기 위해서는 모델은 입력 이미지와 텍스트를 모두 이해할 수 있어야만 한다. 이러한 이해를 위해서 모델은 분리되어 있거나 또는 합쳐져 있는 비전을 위한 인코더와 언어를 위한 인코더로 구성되어 있다.

한편 이러한 입력과 출력의 형태는 다양할 수 있다. 아래는 다양한 형태의 입력 예시이다:
- 텍스트-이미지 검색 (Text-Image Retrieval): 자연어 텍스트로부터의 이미지 검색
- 문장 채우기: 입력 이미지와 자연어 문장으로부터 객체 검출, e.g. A **young person** swings a **bat**.
- VQA(Visual Question Answering): 입력 이미지와 자연어 질문으로부터 정답을 추출
- 이미지 캡셔닝: 주어진 이미지로부터 설명문을 생성
- 이미지와 텍스트로부터 소셜 미디어의 혐오 발언 검출

## 학습 전략
일반적으로 비전-언어 모델은 다음과 같은 3가지 핵십 요소로 구성된다:
- 이미지 인코더 (Image Encoder)
- 텍스트 인코더 (Text Encoder)
- 두 인코더의 정보를 결합할 수 있는 전략

이 핵심 요소들은 모델 구조와 학습 전략을 고려하여 설계된 손실 함수 등을 통해서 서로 밀접하게 연관되어 있다. 비전-언어 모델의 연구 분야는 새로운 분야임에도 불구하고 이러한 모델 설계는 해가 지날수록 크게 변화되고 있다. 과거의 연구들은 사람이 직접 수작업한 이미지 묘사 기술과 사전 훈련된 단어 임베딩 벡터, 또는 단어 빈도수 기반의 TF-IDF 특징을 활용하였다. 반면 최근 연구들은 이미지와 텍스트 특징을 독립적 또는 통합적으로 학습시키는 방식의 Transformer 기반의 이미지 인코더와 텍스트 인코더를 활용한다. 이러한 최신 모델들은 하위 작업들을 수행하기 위해서 전략적으로 사전 훈련이 되었다.

이번 섹션에서는 이러한 비전-언어 모델 사전 훈련의 일반적인 전략들을 확인해볼 것이다. 또한 이러한 전략들에 특화되어 있거나 또는 일반적으로 널리 사용될 수 있는 추가적인 것들도 함께 다뤄볼 것이다.

사전 훈련의 전략을 커버하기 위하여 다음의 주제들을 다룰 예정이다:
- **Constrastive Learning**: Constrastive 방식을 통해서 이미지와 텍스트를 통합 특징 공간에 임베딩하는 방법론
- **PrefixLM**: 언어 모델에 Prefix로 이미지를 입력하는 방식을 통해서 이미지와 텍스트 임베딩을 통합적으로 학습하는 방법론
- **Cross Attention을 통한 멀티모달 결합**: Cross Attention 메커니즘을 통한 비전 정보를 언어 모델의 계층으로 결합하는 방법론
- **MLM / ITM**: 마스크 언어 모델 및 이미지-텍스트 매칭 방식을 활용하여 이미지 일부를 텍스트와 결합하는 방법론
- **No Tranining**: 각각의 비전 모델과 언어 모델을 따로 사용하여 반복적인 최적화를 수행하는 방법론

여기 나와있는 목록은 현재 존재하는 모든 방법론을 나열한 것이 아니라 빠진 것이 있을 수 있으니 더 자세한 사항은 [링크](https://arxiv.org/abs/2210.09263)를 참고하면 좋다.

### Constrastive Learning

![](assets/img/2024-01-03-vlm-01/contrastive-learning.png)

*(출처: [https://openai.com/research/clip](https://openai.com/research/clip))*

`Constrastive Learning`은 컴퓨터 비전 분야에서 널리 사용되었던 사전 훈련 방법론이며 또한 비전-언어 모델의 사전 훈련에도 매우 효과적이라는 것이 증명된 방법론이다. 최근 연구들인 [CLIP](https://arxiv.org/abs/2103.00020), [CLOOB](https://arxiv.org/abs/2110.11316), [ALIGN](https://arxiv.org/abs/2102.05918), [DeCLIP](https://arxiv.org/abs/2110.05208) 등은 이미지 인코더와 텍스트 인코더를 거대 데이터와 Constrastive 손실 함수를 통해서 통합적으로 학습하여 비전과 언어 양상 사이를 연결하였다. 여기서 사용된 거대 데이터는 (이미지, 설명문) 쌍으로 구성된다. Constrastive Learning은 입력 이미지와 입력 텍스트를 동일한 특징 공간으로 매핑하고 둘이 매치가 된다면 거리를 가깝게, 그렇지 않다면 거리를 멀게끔 학습시키는 방법론이다.

CLIP에서는 특징 공간에서의 거리를 두 임베딩 사이의 코사인 유사도로 정의하였고 ALIGN이나 DeCLIP 등의 연구는 잡음이 섞인 데이터셋을 고려하여 자신들만의 메트릭을 따로 정의하였다.

[LiT](https://arxiv.org/abs/2111.07991)라는 또다른 연구는 사전 훈련된 CLIP의 이미지와 텍스트 인코더를 활용하여 이미지 인코더는 그대로 둔 채로 텍스트 인코더만을 파인 튜닝하는 간단한 방법을 제안했다. 저자들은 이 아이디어를 다음과 같이 해석한다: *텍스트 인코더로 하여금 이미지 인코더로부터의 이미지 임베딩을 더 잘 읽을 수 있도록 하는 방법이다.* 이 접근은 CLIP에 비해서 더 효과적이고 데이터 효율성이 좋다는 것이 증명됐다.

[FLAVA](https://arxiv.org/abs/2112.04482) 등의 다른 연구들은 Contrastive Learning과 다른 사전 훈련 방법론들을 조합하여 비전 및 언어 임베딩을 획득하는 방법론을 제안했다.

### PrefixLM

![](assets/img/2024-01-03-vlm-01/prefixLM.png)

*(출처: [https://arxiv.org/abs/2108.10904](https://arxiv.org/abs/2108.10904))*

그 다음 비전-언어 모델 학습 방법론은 `PrefixLM` 방법론이다. [SimVLM](https://arxiv.org/abs/2108.10904), [VirTex](https://arxiv.org/abs/2006.06666v3) 등의 모델들이 PrefixLM 방법론을 적용한 모델들이다. 이 모델들은 Transformer 인코더, Transformer 디코더로 구성된 통합된 멀티모달 구조를 가지고 있으며 Autoregressive 언어 모델의 형태와 매우 유사하다.

조금 더 자세히 살펴보도록 하자. PrefixLM 방법론에서 언어 모델들은 보통 주어진 입력 문장을 Prefix로 하여 그 다음 토큰을 예측하도록 학습된다. 예를 들면, "A man is standing at the corner"라는 문장이 주어졌을 때 "A man is standing at the"를 Prefix로 삼고 모델로 하여금 "Corner" 등과 같은 토큰 에측을 통해 주어진 Prefix에 대해서 합당한 다음 문장을 생성하도록 학습시킨다.

`Visual Transformer`(ViT)는 이미지는 여러개의 패치로 나눠서 패치들을 순차적으로 입력받아 처리하는 모델이다. 이 모델을 활용하여 SimVLM은 Prefix로 이미지 패치 시퀀스와 Prefix에 해당하는 텍스트 시퀀스를 연결하여 인코더에 입력하고 디코더는 이 입력을 바탕으로 이어지는 텍스트를 생성하도록 한다. 위의 그림은 이 방법론을 설명하기 위한 그림이다. SimVLM은 이미지 패치 없이 텍스트만으로 구성된 데이터셋을 통해서 사전 훈련되고 이후에 이미지-텍스트 데이터셋을 통해 파인 튜닝이 수행된다. 이 모델들은 이미지 캡셔닝, VQA 등의 태스크에 활용된다.

비전 정보를 언어 모델에 결합한 형태의 통합 멀티모달 구조를 활용하는 모델들은 이미지를 처리하는 태스크에 매우 우수한 성능을 보인다. 하지만 PrefixLM 방법론을 활용하는 모델들은 그 활용이 이미지 캡셔닝과 관련된 태스크에 한정된다는 단점이 있다.

**Frozen PrefixLM**

![](assets/img/2024-01-03-vlm-01/frozen-prefixLM.png)

*(출처: [https://huggingface.co/blog/vision_language_pretraining](https://huggingface.co/blog/vision_language_pretraining))*

비전 정보를 언어 모델에 결합하는 것은 효율적이지만 사전 훈련된 언어 모델을 파인 튜닝 없이 활용하는 것이 더 좋은 성능을 가질 수도 있다. 이러한 방식으로 언어 모델을 고정하고 그에 맞는 이미지 임베딩을 학습하는 방법론도 있다. 이 방법론을 `Frozen PrefixLM`이라고 한다.

[Frozen](https://arxiv.org/abs/2106.13884), [ClipCap](https://arxiv.org/abs/2111.09734)은 이러한 Frozen PrefixLM 방법론을 활용하는 대표적인 모델이다. 이들은 오직 이미지 인코더의 파라미터만을 학습하여 고정된 언어 모델의 Prefix로 활용될 수 있는 이미지 임베딩을 생성하도록 학습된다. Frozen과 ClipCap 두 모델 모두 이미지-텍스트(설명문) 데이터셋에 학습되어 주어진 이미지 임베딩과 Prefix 텍스트에 대해 설명문의 다음 토큰을 예측하도록 학습한다.

![](assets/img/2024-01-03-vlm-01/flamingo.png)

[MAPL](https://arxiv.org/abs/2210.07179), [Flamingo](https://arxiv.org/abs/2204.14198)는 사전 훈련된 비전 인코더와 언어 모델을 모두 고정한다. Flamingo는 Perceiver Resampler 모듈을 사전 훈련된 모델의 상단에 추가하고, 새로운 Cross-Attention 계층을 사전 훈련된 언어 모델의 계층 사이사이에 집어넣어서 비전 조건을 언어 모델에 부여한다. 위 그림은 Flamingo에 대한 설명을 나타내는 그림이다.

Frozen PrefixLM 방법론의 장점은 멀티모달 데이터셋을 활용하기 어려운 특히 유용하다는 점이다.

### Cross Attention을 통한 멀티모달 결합

![](assets/img/2024-01-03-vlm-01/cross-attn.png)

*(출처: [https://lilianweng.github.io/posts/2022-06-09-vlm/](https://lilianweng.github.io/posts/2022-06-09-vlm/))*

멀티모달 태스크에 사전 학습된 언어 모델을 활용하는 또 다른 접근은 직접적으로 비전 데이터를 언어 모델 디코더에 `Cross Attention`을 활용하여 직접 시각 정보를 결합하는 것이다. [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), [Flamingo](https://arxiv.org/abs/2204.14198) 등의 모델은 이러한 방법론을 통해서 이미지 캡셔닝, VQA 태스크에 활용하도록 학습된다. 이러한 모델들의 목표는 멀티모달 데이터셋이 충분하지 않은 경우에 대응하기 위하여 텍스트 생성 기능과 효율적인 시각 정보 처리 사이의 균형을 맞추는 것이다.

VisualGPT와 같은 모델은 비전 인코더로 하여금 이미지를 임베딩하고 이 이미지 임베딩을 사전 훈련된 언어 모델의 Cross-Attention 계층에 입력하여 이미지에 대한 설명문을 생성하도록 한다. [FIBER](https://arxiv.org/abs/2206.07643)라는 최근 연구는 비전과 언어 백본(Backbone) 모델에 Gating Mechanism이 담긴 Cross-Attention 계층을 삽입하여 더 효율적인 멀티모달 결합과 이미지-텍스트 검색과 Open Vocabulary 객체 검출 등의 하위 태스크를 가능하도록 하였다.

### 마스크 언어 모델링 / 이미지-텍스트 매칭
또 다른 종류의 비전-언어 모델은 마스크 언어 모델과 이미지-텍스트 매칭 태스크의 조합을 사용하여 이미지의 특정 부분을 텍스트와 일치시켜 찾아내고, VQA, Visual Commonsense Reasoning, 텍스트 기반 이미지 검색, 텍스트 기반 객체 검출 등의 하위 태스크를 수행할 수 있다. 이러한 사전 훈련 전략을 활용하는 모델들로는 [VisualBERT](https://arxiv.org/abs/1908.03557), [FLAVA](https://arxiv.org/abs/2112.04482), [ViLBERT](https://arxiv.org/abs/1908.02265), [LXMERT](https://arxiv.org/abs/1908.07490), [BridgeTower](https://arxiv.org/abs/2206.08657) 등이 있다.

![](assets/img/2024-01-03-vlm-01/vilbert.png)

*(출처: [https://arxiv.org/pdf/1908.02265.pdf](https://arxiv.org/pdf/1908.02265.pdf))*

이러한 종류의 사전 훈련 전략이 어떤 의미인지를 자세히 살펴보도록 하자. 부분적으로 마스킹된 설명문이 주어졌을 때, 마스크 언어 모델은 이 설명문과 일치하는 이미지에 대해서 마스킹된 단어를 예측한다. 이런 방식의 마스크 언어 모델의 목적 함수는 바운딩 박스가 포함된 멀티모달 데이터셋, 또는 입력 텍스트의 부분에 대한 Region Proposal이 가능한 객체 검출 모델 활용이 필요하다.

이미지-텍스트 매칭 태스크에서는 이미지와 설명문 쌍이 주어졌을 때, 이 설명문이 주어진 이미지를 설명하고 있는지 여부를 예측한다. Negative 샘플은 데이터셋에서 임의로 추출된다. 마스크 언어 모델 태스크와 이미지-텍스트 매칭 태스크는 멀티모달 모델의 사전 훈련 과정에서 조합된다. 예를 들면 VisualBERT 모델은 사전 훈련된 객체 검출 모델, 여기서는 [Faster-RCNN](https://arxiv.org/abs/1506.01497)으로 하여금 객체들을 검출하도록 사용하는 BERT 형태의 구조를 제안한다. 이 모델은 암묵적으로 Self-Attention을 통해서 입력 텍스트의 일부분과 입력 이미지의 Region을 일치시키도록 사전 훈련 과정에서 마스크 언어 모델과 이미지-텍스트 매칭 태스크를 조합한다.

다른 연구인 FLAVA는 이미지 인코더, 텍스트 인코더, 그리고 멀티모달 추론을 위한 이미지와 텍스트 표현의 결합과 일치를 수행하는 멀티모달 인코더로 구성된다. 이 모든 구성 요소들은 모두 Transformer 기반이다. 학습을 위해서 FLAVA는 다양한 태스크를 결합하여 사전 훈련에 사용된다. 이 태스크들은 마스크 언어 모델, 이미지-텍스트 매칭, 마스크 이미지 모델(Masked-Image Modeling), Contrastive Learning이다.

## 참고 자료
- [https://huggingface.co/blog/vision_language_pretraining](https://huggingface.co/blog/vision_language_pretraining)
- [https://lilianweng.github.io/posts/2022-06-09-vlm/](https://lilianweng.github.io/posts/2022-06-09-vlm/)
- [Vision-Language Pre-training: Basics, Recent Advances, and Future Trends](https://arxiv.org/abs/2210.09263)
- [CLIP: Connecting text and images](https://openai.com/research/clip)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP](https://arxiv.org/abs/2110.11316)
- [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)
- [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](https://arxiv.org/abs/2110.05208)
- [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991)
- [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482)
- [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://arxiv.org/abs/2108.10904)
- [VirTex: Learning Visual Representations from Textual Annotations](https://arxiv.org/abs/2006.06666v3)
- [Multimodal Few-Shot Learning with Frozen Language Models](https://arxiv.org/abs/2106.13884)
- [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734)
- [MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting](https://arxiv.org/abs/2210.07179)
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)
- [VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning](https://arxiv.org/abs/2102.10407)
- [A Frustratingly Simple Approach for End-to-End Image Captioning](https://arxiv.org/abs/2201.12723)
- [Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone](https://arxiv.org/abs/2206.07643)
- [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557)
- [ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265)
- [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)
- [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657)
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)

## 수정 사항
- 2024.01.02
    - 최초 게제