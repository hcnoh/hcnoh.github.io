---
layout: post
use_math: true
title: "[비전-언어 모델] 2. Vision Transformer"
date: 2024-01-17 00:00:00
tagline: "비전-언어 모델에서 이미지 및 비디오 인코더로 주로 활용되는 Transformer 모델인 Vision Transformer에 대한 소개"
categories:
- 비전-언어 모델 스터디
tags:
- deep learning
- natural language processing
- computer vision
- vision-language models
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-17-vlm-02
---

이번 포스트에서는 비전-언어 모델에서 이미지 또는 비디오를 인코딩하기 위해서 주로 활용되는 모델인 `Vision Transformer`에 대해서 설명한다. `Transformer` 모델은 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)에서 자세히 확인할 수 있듯이 자연어 처리 분야에서 처음으로 활용되었다. Transformer의 가치는 `Self-Attention 메커니즘`에서 찾을 수 있는데 Vision Transformer는 이미지를 인코딩하는 과정에서 이 Self-Attention 메커니즘을 효율적으로 활용하였다는 점이 핵심이다. 이번 포스트를 통해서 이미지 인코딩에 Self-Attention을 어떻게 활용하였는지 알아보도록 하자.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## Vision Transformer 소개
`Transformer` 모델은 자연어 처리 분야에서 사실상 표준과 같은 모델이 되었다. 하지만 컴퓨터 비전 분야에서는 현재까지 잘 활용되지 못하였었다. 기존 컴퓨터 비전 분야의 모델은 대부분 CNN에 의존한 모델들이 대부분이었다. `Vision Transformer` 연구의 저자들은 이러한 CNN 의존 모델들과는 다르게 이미지를 분할하여 패치들을 만들고 이 패치들의 시퀀스를 Transformer 구조에 직접 입력하는 방식으로 Vision Transformer의 방법론을 구상했다.

![](assets/img/2024-01-17-vlm-02/vit-arch.png)

위 그림은 Vision Transformer의 구조를 나타내는 그림이다. 우선 이미지를 고정된 크기의 패치들로 나눈다. 그 다음 선형 계층을 통해서 각 패치들을 임베딩한 후 위치 임베딩을 합한다. 그 결과 생성된 벡터의 시퀀스를 표준 Transformer 인코더에 입력한다. 이미지 분류를 수행하기 위해서 자연어 처리에서의 문장 분류 접근 방식과 마찬가지로 추가적인 학습 가능한 토큰을 벡터 시퀀스 맨 앞에 붙여서 활용한다. 이 토큰의 임베딩 결과를 활용하여 이미지 분류를 수행한다.

## Vision Transformer 구조 상세
이 과정을 좀 더 자세히 기술해보도록 하자. 먼저 2D 이미지 $$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$$를 평평한 2D 패치 시퀀스로 변환해야 한다. 변환된 2D 패치 시퀀스 $$\mathbf{x}_p$$는 다음과 같은 형태가 된다:

$$
\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}.
$$

여기서 $$(H, W)$$는 원본 이미지의 사이즈 정보(높이/너비)이고, $$C$$는 원본 이미지의 채널 수이다. 그리고 $$(P, P)$$는 각각의 이미지 패치의 사이즈 정보이다. 또한 $$N$$은 결과 패치들의 개수이며 다음과 같이 계산된다:

$$
N = HW / P^2.
$$

즉, 이 변환 과정을 통해서 모든 이미지는 고정된 크기 $$(P, P)$$인 패치들의 시퀀스로 변환되는 것이다. 이렇게 변환된 패치 시퀀스의 각 패치들은 훈련 가능한 선형 변환 $$\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$$을 통해 $$D$$차원의 벡터로 변환된다:

$$
\mathbf{x}_p \mathbf{E} = \left[ \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right].
$$

여기서 결과물 패치의 선형 변환 $$\mathbf{x}_p^i \mathbf{E}$$을 `패치 임베딩`으로 정의한다. 

BERT의 [class] 토큰과 유사하게 학습 가능한 임베딩 $$\mathbf{x}_\text{class} \in \mathbb{R}^D$$를 패치 임베딩 시퀀스의 맨 앞에 붙인다:

$$
\mathbf{z}_0 = \left[ \mathbf{x}_\text{class} ; \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right], \ \mathbf{z}_0^0 = \mathbf{x}_\text{class}.
$$

[class] 토큰은 위에서 설명하였듯이 이 토큰의 Transformer 인코더의 출력 $$\mathbf{z}_L^0$$ 이미지의 표현 벡터 $$\mathbf{y}$$로 활용될 것이다. 이를 활용하는 분류 계층은 사전 훈련 과정에서는 MLP(Multi-Layer Perceptron)을, 파인 튜닝 과정에서는 단일 선형 계층을 사용한다.

Transformer에서와 마찬가지로 Vision Transformer에서도 위치 임베딩이 패치 임베딩에 합해짐으로써 위치 정보를 보전하게 된다. 여기서는 특별환 변형없이 표준적인 학습 가능한 1D 위치 임베딩을 활용한다. 이 과정을 정리하면 패치 임베딩 시퀀스 계산 과정이 아래와 같이 변경된다:

$$
\mathbf{z}_0 = \left[ \mathbf{x}_\text{class} ; \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right] + \mathbf{E}_\text{pos}, \quad \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \mathbf{E}_\text{pos} \in \mathbb{R}^{(N+1) \times D}.
$$

## 정리

## 참고 자료
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## 수정 사항
- 2024.01.12
    - 최초 게제