---
layout: post
use_math: true
title: "[비전-언어 모델] 2. Vision Transformer"
date: 2024-01-17 00:00:00
tagline: "비전-언어 모델에서 이미지 및 비디오 인코더로 주로 활용되는 Transformer 모델인 Vision Transformer에 대한 소개"
categories:
- 비전-언어 모델 스터디
tags:
- deep learning
- natural language processing
- computer vision
- vision-language models
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-17-vlm-02
---

이번 포스트에서는 비전-언어 모델에서 이미지 또는 비디오를 인코딩하기 위해서 주로 활용되는 모델인 `Vision Transformer`(ViT)에 대해서 설명한다. `Transformer` 모델은 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)에서 자세히 확인할 수 있듯이 자연어 처리 분야에서 처음으로 활용되었다. Transformer의 가치는 `Self-Attention 메커니즘`에서 찾을 수 있는데 ViT는 이미지를 인코딩하는 과정에서 이 Self-Attention 메커니즘을 효율적으로 활용하였다는 점이 핵심이다. 이번 포스트를 통해서 이미지 인코딩에 Self-Attention을 어떻게 활용하였는지 알아보도록 하자.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## Vision Transformer 소개
`Transformer` 모델은 자연어 처리 분야에서 사실상 표준과 같은 모델이 되었다. 하지만 컴퓨터 비전 분야에서는 현재까지 잘 활용되지 못하였었다. 기존 컴퓨터 비전 분야의 모델은 대부분 CNN에 의존한 모델들이 대부분이었다. `Vision Transformer` (ViT) 연구의 저자들은 이러한 CNN 의존 모델들과는 다르게 이미지를 분할하여 패치들을 만들고 이 패치들의 시퀀스를 Transformer 구조에 직접 입력하는 방식으로 ViT의 방법론을 구상했다.

![](assets/img/2024-01-17-vlm-02/vit-arch.png)

위 그림은 ViT의 구조를 나타내는 그림이다. 우선 이미지를 고정된 크기의 패치들로 나눈다. 그 다음 선형 계층을 통해서 각 패치들을 임베딩한 후 위치 임베딩을 합한다. 그 결과 생성된 벡터의 시퀀스를 표준 Transformer 인코더에 입력한다. 이미지 분류를 수행하기 위해서 자연어 처리에서의 문장 분류 접근 방식과 마찬가지로 추가적인 학습 가능한 토큰을 벡터 시퀀스 맨 앞에 붙여서 활용한다. 이 토큰의 임베딩 결과를 활용하여 이미지 분류를 수행한다.

## Vision Transformer 구조 상세
이 과정을 좀 더 자세히 기술해보도록 하자. 먼저 2D 이미지 $$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$$를 평평한 2D 패치 시퀀스로 변환해야 한다. 변환된 2D 패치 시퀀스 $$\mathbf{x}_p$$는 다음과 같은 형태가 된다:

$$
\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}.
$$

여기서 $$(H, W)$$는 원본 이미지의 사이즈 정보(높이/너비)이고, $$C$$는 원본 이미지의 채널 수이다. 그리고 $$(P, P)$$는 각각의 이미지 패치의 사이즈 정보이다. 또한 $$N$$은 결과 패치들의 개수이며 다음과 같이 계산된다:

$$
N = HW / P^2.
$$

즉, 이 변환 과정을 통해서 모든 이미지는 고정된 크기 $$(P, P)$$인 패치들의 시퀀스로 변환되는 것이다. 이렇게 변환된 패치 시퀀스의 각 패치들은 훈련 가능한 선형 변환 $$\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$$을 통해 $$D$$차원의 벡터로 변환된다:

$$
\mathbf{x}_p \mathbf{E} = \left[ \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right].
$$

여기서 결과물 패치의 선형 변환 $$\mathbf{x}_p^i \mathbf{E}$$을 `패치 임베딩`으로 정의한다. 

BERT의 [class] 토큰과 유사하게 학습 가능한 임베딩 $$\mathbf{x}_\text{class} \in \mathbb{R}^D$$를 패치 임베딩 시퀀스의 맨 앞에 붙인다:

$$
\mathbf{z}_0 = \left[ \mathbf{x}_\text{class} ; \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right], \ \mathbf{z}_0^0 = \mathbf{x}_\text{class}.
$$

[class] 토큰은 위에서 설명하였듯이 이 토큰의 Transformer 인코더의 출력 $$\mathbf{z}_L^0$$ 이미지의 표현 벡터 $$\mathbf{y}$$로 활용될 것이다. 이를 활용하는 분류 계층은 사전 훈련 과정에서는 MLP(Multi-Layer Perceptron)을, 파인 튜닝 과정에서는 단일 선형 계층을 사용한다.

Transformer에서와 마찬가지로 ViT에서도 위치 임베딩이 패치 임베딩에 합해짐으로써 위치 정보를 보전하게 된다. 여기서는 특별환 변형없이 표준적인 학습 가능한 1D 위치 임베딩을 활용한다. 이 과정을 정리하면 패치 임베딩 시퀀스 계산 과정이 아래와 같이 변경된다:

$$
\mathbf{z}_0 = \left[ \mathbf{x}_\text{class} ; \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right] + \mathbf{E}_\text{pos}, \quad \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \mathbf{E}_\text{pos} \in \mathbb{R}^{(N+1) \times D}.
$$

최종적인 ViT의 구조를 수식으로 정리하면 아래와 같다:

$$
\begin{array}{rll}
\mathbf{z}_0
& = \left[ \mathbf{x}_\text{class} ; \mathbf{x}_p^1 \mathbf{E} ; \mathbf{x}_p^2 \mathbf{E} ; \cdots ; \mathbf{x}_p^N \mathbf{E} \right] + \mathbf{E}_\text{pos},
\quad & \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \mathbf{E}_\text{pos} \in \mathbb{R}^{(N+1) \times D}. \\
\mathbf{z}_l'
& = \text{MSA} \left( \text{LN} \left( \mathbf{z}_{l - 1} \right) \right) + \mathbf{z}_{l - 1},
\quad & l = 1, 2, \cdots, L. \\
\mathbf{z}_l
& = \text{MLP} \left( \text{LN} \left( \mathbf{z}_l' \right) \right) + \mathbf{z}_l',
\quad & l = 1, 2, \cdots, L. \\
\mathbf{y}
& = \text{LN} \left( \mathbf{z}_L^0 \right).
\end{array}
$$

여기서 $$\text{MSA}$$는 Multi-Head Self-Attention 계층을, $$\text{MLP}$$는 GELU 비선형 활성 함수를 포함한 계층 2개의 MLP 블록을, $$\text{LN}$$은 Layer Normalization을 말한다. Transformer 구조에 대해서 자세히 알고 싶으면 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)를 참고하면 된다.

## Vision Transformer의 이점
ViT는 기존 CNN 기반의 이미지 인코더 모델들에 비해서 다음과 같은 이점이 있다고 한다.

### 귀납적 편향 (Inductive Bias)
`귀납적 편향`(Inductive Bias), 또는 `학습 편향`(Learning Bias)이란 학습 알고리즘에서 학습자, 즉 모델이 주어진 입력에 대해서 출력을 예측할 때 사용하는 추가적인 가정들의 집합을 말하며, 이 집합들은 학습 과정에서 사용된 데이터를 통해서는 발견할 수 없었던 것들을 말한다. 즉, 모델이 더 좋은 성능을 위해서 학습이 아닌 방식으로 추가적인 가정을 하고 있다면 이는 귀납적 편향의 하나로 볼 수 있다.

귀납적 편향의 대표적인 예시는 CNN에서 확인할 수 있다. CNN은 크게 2가지의 귀납적 편향을 보유하고 있다. 첫 번째는 `Locality`이다. Locality는 이름에서 확인할 수 있듯이 이미지의 근접 픽셀끼리의 종속성을 의미한다. 두 번째는 `Translational Invariance`이다. 이는 이미지의 객체 위치가 변경되어도 그 객체는 동일한 객체로 인식되어야 한다는 것이다. 이 두 가지 귀납적 편향은 CNN이 지금까지도 이미지 분석에 매우 훌륭한 성능을 보이는 것을 뒷받침하였다.

반면 ViT는 CNN에 비해서 훨씬 적은 이미지 특화된 귀납적 편향을 지니고 있다. 오직 MLP 계층에서만 Locality 및 Translational Invariance가 있고, MSA 계층은 Global하다. MLP는 각각의 패치 정보들에 대해서 처리하기 때문에 Locality를 반영하고 있고, 모든 패치에 대해서 동일한 MLP 계층 연산이 적용되기 때문에 Translational Invariance를 반영하게 되는 것이다. 반면 MSA 계층은 전체 패치들 사이의 Attention을 수행하는 계층이기 때문에 Globality를 반영하고 있다.

이런 식으로 ViT는 CNN에 비해서 적은 귀납적 편향을 지니고 있으며 이에 따라 CNN에 비해 학습에 더 많은 데이터를 필요로 하게 된다. 또한 CNN은 Locality를 잘 반영하고 있기 때문에 지엽적인 정보 처리에 능하다는 장점이 있으나 ViT는 반면 지엽적인 정보 처리보다는 Globality 성질을 통해서 이미지의 전체적인 정보 처리에 능하다고 볼 수 있다.

### 하이브리드 구조
위에서 설명하였듯 CNN은 ViT에 비해서 또 다른 장점이 있다는 것을 알 수 있다. 따라서 CNN의 장점을 함께 누릴 수 있는 ViT의 `하이브리드 구조`를 제안할 수 있다. 이는 Raw 이미지 패치 대신 CNN의 특징 맵(Feature Map)을 통해서 입력 시퀀스를 구성하는 방식으로 구현한다. 이러한 하이브리드 구조에서는 CNN의 특징 맵으로부터 추출된 패치들에 대해서 패치 임베딩 $$\mathbf{E}$$가 적용된다.

## 파인 튜닝 및 고해상도 이미지
일반적으로 ViT의 사전 훈련은 거대 데이터셋에 대해서 수행되며 파인 튜닝은 좀 더 작은 하위 태스크 데이터셋에 대해서 수행된다. 파인 튜닝을 위해서 사전 훈련된 Prediction Head를 제거하고 가중치가 0으로 초기화된 $$D \times K$$ 크기의 순방향 계층을 붙인다. 여기서 $$K$$는 하위 태스크의 클래스 수이다.

또한 이전 연구들에 의하면 파인 튜닝 과정에서는 사전 훈련 과정에서보다 더 고해상도인 이미지를 활용하는 경우에 이점이 있다고 한다. 이를 위하여 더 고해상도 이미지를 모델이 입력시키게 되면 모델의 패치 사이즈는 고정되어있기 때문에 모델이 처리해야하는 시퀀스 길이가 더욱 길어지게 된다. ViT 모델은 임의의 시퀀스 길이를 다룰 수 있는 구조이지만 대신 사전 훈련된 위치 임베딩은 더 이상 사용할 수 없게 된다.

이 문제를 해결하기 위하여 ViT에서는 위치 임베딩에 대해서 `2D Interpolation`을 수행하는 방식으로 고해상도 이미지에 대응한다. 이러한 해상도 조절과 패치 추출은 2D 이미지 구조에 대한 귀납적 편향을 수동으로 ViT 모델에 주입하는 것과 같다고 볼 수 있다.

## 싫험 및 결과
우선 저자들은 ViT의 성능을 비교하기 위하여 ResNet과 ViT의 성능을 비교하였고, 추가적으로 하이브리드 구조의 성능을 확인하였다. 각각의 모델들에 대한 데이터 요구 사항을 확인하기 위하여 다양한 크기의 데이터셋을 사전 훈련시켰고 다양한 종류의 벤치마크에 대한 평가를 진행하였다. ViT는 연산 비용을 고려하였을때 가장 저렴한 사전 훈련 비용으로 대부분의 인식 벤치마크 테스트에 대해서 SOTA 성능을 달성하였다.

### 실험 세팅
모델의 확장성을 평가하기 위하여 저자들은 `ILSVRC-2012 이미지넷 데이터셋`을 활용하였다. 이 데이터셋은 총 1000개의 클래스와 130만 장의 이미지로 구성되어 있다. 이 데이터셋을 활용하기 위하여 중복되는 데이터셋을 제거하였는데 그 방법은 [이전 연구(Are we done with ImageNet?)](https://arxiv.org/abs/2006.07159)를 참고하여 동일하게 수행하였다. 이 이전 연구의 내용을 간단히 정리하면 아래와 같다:

- 기존 이미지넷 벤치마크는 컴퓨터 비전 분야에서 훌륭한 벤치마크 역할을 수행하였으나 그 특징적인 부분이 존재하였고 연구 집단은 여기에 과적합되는 양상을 보이는 문제가 있다.
- 기존 이미지넷 데이터셋의 문제점 (1):
    - 각 이미지에 무조건 하나의 레이블만이 존재하는 문제가 있다. 현실 세계에서는 이미지 내부의 다양한 객체들에 관심을 가지는 문제들이 대부분인데 이러한 문제는 현실 문제를 반영하지 못하는 문제점이 있다.
- 기존 이미지넷 데이터셋의 문제점 (2):
    - 레이블 제안에 대한 지나친 규제 사항들이 있다. 이미지넷 어노테이션 파이프라인은 주어진 클래스에 대해서 인터넷에 요청을 날려서 이미지를 획득하고 획득한 이미지에 실제로 주어진 클래스의 객체가 포함되어 있는지 사람이 확인한다. 이 과정은 정확함을 주기도 하지만 부정확함을 주기도 한다. 주어진 클래스에 대해서만 확인하고 다른 잠재적인 클래스는 확인하지 않는 문제가 있기 때문이다.
- 기존 이미지넷 데이터셋의 문제점 (3):
    - 클래스 구분에 대해서 너무 정확하지 못하다. 의미론적으로 동일한 클래스들, 예를 들면 "sunglass"와 "sunglasses", 또는 "labtop"과 "notebook"은 의미론적으로 동일하지만 이미지넷 클래스에서는 다른 것으로 구분되어 있다.
- 해당 연구에서는 이러한 기존 이미지넷 데이터셋의 문제점을 해결하기 위한 새로운 어노테이션 파이프라인을 제안했다.

이렇게 획득한 데이터셋을 바탕으로 ViT는 사전 훈련을 수행했고 몇몇 하위 벤치마크 태스크들에 대해서 전이 학습(Transfer Learning)을 수행하였다. 하위 태스크들의 목록은 아래와 같다:

- 이미지넷 검증용 데이터셋
- CIFAR-10/100
- Oxford-IIIT Pets
- Oxford Flowers-102

또한 추가적으로 VTAB 분류 태스크 19개를 활용하였다. 이 태스크는 태스크별로 1000개의 적은 수의 학습용 데이터셋만을 가지고 다양한 태스크에 전이 학습 능력을 평가하기 위한 태스크이다. 이 태스크들은 총 3개의 카테고리로 분류되어 있다:

- Natural: CIFAR-10과 같은 일반적인 태스크
- Specialized: 의료, 우주, 형상화
- Structured: 기하학적인 이해가 필요한 태스크

ViT 모델은 아래 표의 세팅을 활용하였다.

Model       | Layers    | Hidden Size   | MLP Size  | Heads | Params
--          | --        | --            | --        | --    | --
ViT-Base    | 12        | 768           | 3072      | 12    | 86M
ViT-Large   | 24        | 1024          | 4096      | 16    | 307M
ViT-Huge    | 32        | 1280          | 5120      | 16    | 632M

## Vision Transformer 분석

![](assets/img/2024-01-17-vlm-02/self-attention-analysis.png)

ViT를 좀 더 자세히 분석해보기 위해서는 ViT의 내부 표현을 살펴볼 필요가 있다. 위 그림의 좌측 그림은 ViT의 학습된 첫 번째 계층인 선형 패치 임베딩 필터의 최고 주성분 (Principal Component) 28개를 나타내는 그림이다. 이를 분석하기 위해서 패치 임베딩의 선형 계층의 가중치 행렬에 대한 주성분 분석(Principal Component Analysis)을 수행한 것 같다. 여기서 확인되는 주성분들은 각 패치에 대한 저차원 표현을 위한 기저 함수(Basis Function)의 역할을 하는 것으로 보인다.

선형 계층을 통한 패치 투사 이후에 이 패치 임베딩에 위치 임베딩이 합하여진다. 위 그림의 중앙 그림은 각 위치 임베딩이 다른 위치 임베딩과 유사도가 어떻게 되는지를 나타내는 그림을 위치 임베딩 각각의 위치별로 나타낸 것이다. 이 그림을 통해서 우리는 각 위치 임베딩이 인접한 위치에 해당하는 임베딩들과 유사도가 높은 것을 확인할 수 있다.

Self-Attention은 ViT로 하여금 가장 초반의 계층에서도 전체 이미지 정보를 통합할 수 있도록 한다. 잘 학습된 CNN의 특징 맵을 살펴보면 초반부 계층에서는 특징 맵이 특정 도형 모양 등의 매우 일반적인 특징을 지녔다면 후반부 계층으로 갈 수록 점점 더 구체적인 모양으로 변화하는 것을 확인할 수 있다. 이는 초반부 계층에서는 아주 지엽적인 정보들을 추출하고 후반부 계층으로 갈 수록 점점 더 전체적인 정보들을 추출하도록 모델이 학습되어 진다는 점을 나타낸다고 볼 수 있다.

반면 ViT는 패치 사이사이의 Attention을 통해서 매우 전체적으로 이미지를 관망하고 정보를 취합하게 된다. 가장 상단 좌측의 패치 정보를 인코딩하는 과정에서 가장 하단 우측의 패치 정보를 Attention을 통해서 함께 활용하기 때문이다. ViT의 저자들은 어떤 요소들을 통해서 이러한 Globality를 확인할 수 있는지를 `Attention Distance`를 통해서 확인하였다.

Attention Distance란 주어진 패치의 Self-Attention 점수를 바탕으로 전체 패치들과의 거리들을 이 Attention 점수와 가중합을 계산한 것이다. 만약 더 멀리있는 패치들의 Attention 점수가 높다면 Attention Distance는 더 높게 계산될 것이다. 즉, Attention Distance가 높다면 해당 Attention을 통해 현재 패치 정보를 인코딩하는 과정에서 더 멀리있는 패치들의 정보를 활용한다고 해석할 수 있다.

위 그림의 우측 그래프는 계층의 깊이가 증가하면서 Attention Distance의 평균이 어떻게 변화하는지를 나타내는 그래프이다. 이 그래프를 살펴보면 전반적으로 후반부 계층으로 갈 수록 Attention Distance가 증가하는 것을 확인할 수 있다. 이는 CNN과 마찬가지로 후반부 계층에서 좀 더 전체적인 정보들을 추출하고 있다는 것이라고 볼 수 있다. 하지만 초반부 계층에서도 몇몇의 Attention Head들은 Attention Distance가 매우 높다는 것을 확인할 수 있다. 즉, 초반부 계층에서도 일부 Head들을 통해서 전체적인 정보 통합이 수행되고 있다고 해석할 수 있다.

## 정리

## 참고 자료
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [Wikipedia: Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias)
- [What is inductive bias?](https://dacon.io/en/forum/405840)
- [Transformer는 Inductive Bias이 부족하다라는 의미는 무엇일까?](https://moon-walker.medium.com/transformer%EB%8A%94-inductive-bias%EC%9D%B4-%EB%B6%80%EC%A1%B1%ED%95%98%EB%8B%A4%EB%9D%BC%EB%8A%94-%EC%9D%98%EB%AF%B8%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C-4f6005d32558)
- [Are we done with ImageNet?](https://arxiv.org/abs/2006.07159)

## 수정 사항
- 2024.01.17
    - 최초 게제