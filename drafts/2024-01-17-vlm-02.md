---
layout: post
use_math: true
title: "[비전-언어 모델] 2. Vision Transformer"
date: 2024-01-17 00:00:00
tagline: "비전-언어 모델에서 이미지 및 비디오 인코더로 주로 활용되는 Transformer 모델인 Vision Transformer에 대한 소개"
categories:
- 비전-언어 모델 스터디
tags:
- deep learning
- natural language processing
- computer vision
- vision-language models
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2024-01-17-vlm-02
---

이번 포스트에서는 비전-언어 모델에서 이미지 또는 비디오를 인코딩하기 위해서 주로 활용되는 모델인 `Vision Transformer`에 대해서 설명한다. `Transformer` 모델은 [링크](https://hcnoh.github.io/2023-02-25-nlp-03)에서 자세히 확인할 수 있듯이 자연어 처리 분야에서 처음으로 활용되었다. Transformer의 가치는 `Self-Attention 메커니즘`에서 찾을 수 있는데 Vision Transformer는 이미지를 인코딩하는 과정에서 이 Self-Attention 메커니즘을 효율적으로 활용하였다는 점이 핵심이다. 이번 포스트를 통해서 이미지 인코딩에 Self-Attention을 어떻게 활용하였는지 알아보도록 하자.

## 목차
- [정리](#정리)
- [참고 자료](#참고-자료)
- [수정 사항](#수정-사항)

## Vision Transformer 소개
`Transformer` 모델은 자연어 처리 분야에서 사실상 표준과 같은 모델이 되었다. 하지만 컴퓨터 비전 분야에서는 현재까지 잘 활용되지 못하였었다. 기존 컴퓨터 비전 분야의 모델은 대부분 CNN에 의존한 모델들이 대부분이었다. `Vision Transformer` 연구의 저자들은 이러한 CNN 의존 모델들과는 다르게 이미지를 분할하여 패치들을 만들고 이 패치들의 시퀀스를 Transformer 구조에 직접 입력하는 방식으로 Vision Transformer의 방법론을 구상했다.

![](assets/img/2024-01-17-vlm-02/vit-arch.png)

위 그림은 Vision Transformer의 구조를 나타내는 그림이다. 우선 이미지를 고정된 크기의 패치들로 나눈다. 그 다음 선형 계층을 통해서 각 패치들을 임베딩한 후 위치 임베딩을 합한다. 그 결과 생성된 벡터의 시퀀스를 표준 Transformer 인코더에 입력한다. 이미지 분류를 수행하기 위해서 자연어 처리에서의 문장 분류 접근 방식과 마찬가지로 추가적인 학습 가능한 토큰을 벡터 시퀀스 맨 앞에 붙여서 활용한다.

## 정리

## 참고 자료
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## 수정 사항
- 2024.01.12
    - 최초 게제